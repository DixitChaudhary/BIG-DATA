{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C0854138_LE_8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "b334365d-88cb-4e6a-9295-bc7637adf08c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "21f720d8-4071-4485-a75d-f8fc2b83e91d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "72e3362d-0625-4139-e8b2-7d733603b451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "d3c149dd-65c6-40ba-a618-72803ffb9314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "ef30a48f-7c78-4153-8456-4f6e0157bfdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-8\n",
        "learning_rate = 1e-8\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "200dc519-994b-4ba2-8e22-14e56a74d919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.981073\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.981263\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.981450\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.981636\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.961819\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.962201\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.952579\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.953053\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.953523\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.953987\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.934448\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.915103\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.915952\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.916793\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.917625\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.918448\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.899264\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.900271\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.901269\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.902256\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.893233\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.894301\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.895358\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.886404\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.887540\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.878665\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.869878\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.871179\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.862468\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.863843\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.865205\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.866553\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.867887\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.869208\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.850516\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.852011\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.853491\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.854956\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.856406\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.857842\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.859264\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.860671\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.852064\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.853544\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.855008\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.856458\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.847894\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.839415\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.831021\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.822710\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.824483\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.826238\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.807976\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.799896\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.791897\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.793978\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.796039\n",
            "resetting env. episode 68.000000, reward total was -18.000000. running mean: -20.768078\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.760397\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.762793\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.765166\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.767514\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.769839\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.762140\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.764519\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.756874\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.759305\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.751712\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.744195\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.746753\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.729285\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.731993\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.734673\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.737326\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.739953\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.742553\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.745128\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.747676\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.750200\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.732698\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.735371\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.738017\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.740637\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.733230\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.725898\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.708639\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.711553\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.714437\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.717293\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.720120\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.722919\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.725689\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.708433\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.701348\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.704335\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.697291\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.700318\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.703315\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.706282\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.699219\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.702227\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.685205\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.688353\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.691469\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.694555\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.697609\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.700633\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.703627\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.706590\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.689524\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.692629\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.685703\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.688846\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.691957\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.695038\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.688087\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.681207\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.684395\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.687551\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.690675\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.683768\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.686931\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.670061\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.673361\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.666627\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.669961\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.663261\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.666629\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.649962\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.653463\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.656928\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.650359\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.643855\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.617417\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.621242\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.625030\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.628780\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.612492\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.606367\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.610303\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.614200\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.618058\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.621878\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.625659\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.629402\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.623108\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.606877\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.610809\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.614700\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.618553\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.612368\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.616244\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.610082\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.613981\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.617841\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.611663\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.615546\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.619391\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.623197\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.626965\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.620695\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.614488\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.618343\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.612160\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.616038\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.619878\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.603679\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.587642\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.591766\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.595848\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.589890\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.593991\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.578051\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.582270\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.576448\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.580683\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.564876\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.569228\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.563535\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.557900\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.562321\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.546698\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.541231\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.545819\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.550360\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.544857\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.539408\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.544014\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.528574\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.533288\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.537955\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.542576\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.547150\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.551679\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.556162\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.550600\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.555094\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.559543\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.563948\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.568308\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.572625\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.576899\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.571130\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.575419\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.569664\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.563968\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.568328\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.572645\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.566918\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.571249\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.575537\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.579781\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.573984\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.578244\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.582461\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.576637\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.580870\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.585062\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.589211\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.583319\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.587486\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.591611\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.585695\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.579838\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.564039\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.568399\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.572715\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.576988\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.571218\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.575506\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.579751\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.583953\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.578114\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.582333\n",
            "resetting env. episode 245.000000, reward total was -18.000000. running mean: -20.556509\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.560944\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.545335\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.549881\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.554383\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.558839\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.543250\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.527818\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.532540\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.517214\n",
            "resetting env. episode 255.000000, reward total was -18.000000. running mean: -20.492042\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.497122\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.502150\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.497129\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.482158\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.487336\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -20.462463\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.467838\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.453160\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.458628\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.464042\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.469401\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.464707\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.470060\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.475360\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.470606\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.455900\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.461341\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.466728\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.462060\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.467440\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.472765\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.468038\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.473357\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.478624\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.483838\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.468999\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.474309\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.469566\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.444870\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.450422\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.455918\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.461358\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.466745\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.472077\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.477357\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.482583\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.487757\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.482880\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.478051\n",
            "resetting env. episode 295.000000, reward total was -18.000000. running mean: -20.453270\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.458738\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.464150\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.459509\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.464914\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.470264\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.465562\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.470906\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.476197\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.481435\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.486621\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.491755\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.496837\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.491869\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.496950\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.501980\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.506961\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.501891\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.496872\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.501903\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.486884\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.482016\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.477195\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.482423\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.487599\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.482723\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.477896\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.483117\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.478286\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.483503\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.488668\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.483781\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.488943\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.484054\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.489214\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.484321\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.489478\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.484583\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.489738\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.494840\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.499892\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.504893\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.509844\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.504745\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.489698\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.484801\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.489953\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.495054\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.500103\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.505102\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.510051\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.514950\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.519801\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.524603\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.519357\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.524163\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.528922\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.523632\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.528396\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.533112\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.537781\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.542403\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.546979\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.551509\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.545994\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.550534\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.555029\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.559479\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.563884\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.568245\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.572563\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.576837\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.581069\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.575258\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.569505\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.573810\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.578072\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.582292\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.576469\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.580704\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.574897\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.569148\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.563456\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.567822\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.562144\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.556522\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.560957\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.565347\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.569694\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.573997\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.558257\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.552674\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.537148\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.541776\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.536358\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.520995\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.515785\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.500627\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.505621\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.510565\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.515459\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.520304\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.515101\n",
            "resetting env. episode 398.000000, reward total was -17.000000. running mean: -20.479950\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.475151\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.480399\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.485595\n",
            "resetting env. episode 402.000000, reward total was -18.000000. running mean: -20.460739\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.466132\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.471471\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.476756\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.481988\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.487169\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.492297\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.497374\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.502400\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.507376\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.512302\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.507179\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.502108\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.507086\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.512016\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.496895\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.501926\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.486907\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.492038\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.497118\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.502147\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.507125\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.512054\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.516933\n",
            "resetting env. episode 426.000000, reward total was -17.000000. running mean: -20.481764\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.486946\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.492077\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.487156\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -20.462285\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.457662\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.463085\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.468454\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.463770\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.459132\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.464541\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.469895\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.475196\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.460444\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.465840\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.471182\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.476470\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.481705\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.486888\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.492019\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.497099\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.502128\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.507107\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.502036\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.507015\n",
            "resetting env. episode 451.000000, reward total was -17.000000. running mean: -20.471945\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.477226\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.472453\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.477729\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.482952\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.488122\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.493241\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.478308\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.463525\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.458890\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.464301\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.459658\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.465062\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.460411\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.465807\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.471149\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.476437\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.481673\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.476856\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.472088\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.477367\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.482593\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.487767\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.492889\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.487961\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.493081\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.498150\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.493169\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.498237\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.493255\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.498322\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.503339\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.508305\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.513222\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.518090\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.522909\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.507680\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.502603\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.507577\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.502502\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.477477\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.482702\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.487875\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.492996\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.478066\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.483285\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.478453\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.473668\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.458931\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.434342\n",
            "CPU times: user 33min 4s, sys: 12min 38s, total: 45min 42s\n",
            "Wall time: 23min 53s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "cc6ff19e-0de4-4939-fbb7-acca8590f64f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.981073\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.981263\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.971450\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971736\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.962018\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.962398\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.942774\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.943346\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.933913\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.934574\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.935228\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.935876\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.936517\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.937152\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.937780\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.938402\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.939018\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.939628\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.940232\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.940830\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.921421\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.922207\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.922985\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.923755\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.924518\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.925272\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.926020\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.926760\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.927492\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.928217\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.918935\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.899745\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.900748\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.901741\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.902723\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.903696\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.884659\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.885812\n",
            "resetting env. episode 47.000000, reward total was -18.000000. running mean: -20.856954\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.858385\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.859801\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.851203\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.832691\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.834364\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.836020\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.837660\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.839283\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.830891\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.822582\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.814356\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.796212\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.798250\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.800268\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.802265\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.804242\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.796200\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.798238\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.800256\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.802253\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.804231\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.806188\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.808126\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.800045\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.802045\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.804024\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.805984\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -20.777924\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.780145\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.782343\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.774520\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.776775\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.759007\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.761417\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.763803\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.766165\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.768503\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.750818\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.743310\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.745877\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.748418\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.740934\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.743525\n",
            "resetting env. episode 91.000000, reward total was -17.000000. running mean: -20.706089\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.689028\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.692138\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.695217\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.698265\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.691282\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.684369\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.687525\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.690650\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.683744\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.686906\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.690037\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.693137\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.686205\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.689343\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.682450\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.675625\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.678869\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.682080\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.675260\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.668507\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.671822\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.665104\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.668453\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.661768\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.645151\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.648699\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.622212\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -20.595990\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.590030\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.594130\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.578188\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.582407\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.586582\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.590717\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.594809\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.598861\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.602873\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.606844\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.600776\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.594768\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.598820\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.602832\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.596804\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.580836\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.575027\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.579277\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.583484\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.587649\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.591773\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.595855\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.599897\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.603898\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.587859\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.591980\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.596060\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.600100\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.584099\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.578258\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.582475\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.586650\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.580784\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.584976\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.589126\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.593235\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.597303\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.591330\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -20.575416\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.569662\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.573966\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.578226\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.582444\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.586619\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.590753\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.594845\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.578897\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.583108\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.577277\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.581504\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.575689\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.579932\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.584133\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.578292\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.582509\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.576684\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.560917\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.565308\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.559655\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.554058\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.558517\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.562932\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.557303\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.561730\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.546113\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.540651\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.535245\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.519892\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.524694\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.509447\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.504352\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.499309\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.504316\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.509272\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.514180\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.509038\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.513947\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.508808\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.503720\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.508683\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.513596\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.518460\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.523275\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.528043\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.532762\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.527435\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.532160\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.536839\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.541470\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.546056\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.550595\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.555089\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.559538\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.563943\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.568303\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.572620\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.566894\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.551225\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.555713\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.540156\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.544754\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.549307\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.543814\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.538375\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.532992\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.537662\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.542285\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.546862\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.551394\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.535880\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.540521\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.535116\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.539765\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.544367\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.548923\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.553434\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.557900\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.542321\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.546897\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.551429\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.555914\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.560355\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.554752\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.559204\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.553612\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.558076\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.562495\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.566870\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.571201\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.575489\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.579735\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.583937\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.588098\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.592217\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.586295\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.590432\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.584527\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.588682\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.572795\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.577067\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.581297\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.575484\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.579729\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.583932\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.568092\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.572411\n",
            "resetting env. episode 266.000000, reward total was -18.000000. running mean: -20.546687\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.551220\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.555708\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.560151\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.564550\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.568904\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.553215\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.557683\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.552106\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.546585\n",
            "resetting env. episode 276.000000, reward total was -18.000000. running mean: -20.521119\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.525908\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.530649\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.535342\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.539989\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.544589\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.549143\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.543652\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.548215\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.552733\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.547206\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.551734\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.556216\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.560654\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.565048\n",
            "resetting env. episode 291.000000, reward total was -18.000000. running mean: -20.539397\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.544003\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.548563\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.543078\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.527647\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.532370\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.537047\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.531676\n",
            "resetting env. episode 299.000000, reward total was -18.000000. running mean: -20.506359\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.511296\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.496183\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.491221\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.496309\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.491346\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.466432\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.471768\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.477050\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.482280\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.487457\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.492582\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.497657\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.502680\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.507653\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.512577\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.517451\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.522276\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.517054\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.521883\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.516664\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.511498\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.516383\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.521219\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.526007\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.520747\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.515539\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.520384\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.525180\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.529928\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.524629\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.529382\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.534089\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.538748\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.543360\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.547927\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.552447\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.556923\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.561354\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.565740\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.560083\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.564482\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.568837\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.563149\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.567517\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.561842\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.566224\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.560561\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.564956\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.569306\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.563613\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.567977\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.562297\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.556674\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.551108\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.535597\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.540241\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.534838\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.539490\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.544095\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.548654\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.543167\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.537736\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.542358\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.546935\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.541465\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.536051\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.540690\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.545283\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.549831\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.554332\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.538789\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.543401\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.547967\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.552487\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.556962\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.551393\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.555879\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.560320\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.564717\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.569070\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.553379\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.547845\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.532367\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.537043\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.541673\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.546256\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.540793\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.525385\n",
            "resetting env. episode 388.000000, reward total was -18.000000. running mean: -20.500132\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.505130\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.500079\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.505078\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.500027\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.505027\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.509977\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.514877\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.519728\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.524531\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.529286\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.533993\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.518653\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.513466\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.508332\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.503248\n",
            "resetting env. episode 404.000000, reward total was -17.000000. running mean: -20.468216\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.463534\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.468898\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.474209\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.479467\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.484673\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -20.459826\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.465228\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.460575\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.465970\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.471310\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.476597\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.471831\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.477113\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.482341\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.487518\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.492643\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.487716\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.482839\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.488011\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.483131\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.488299\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.483416\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.488582\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.493697\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.498760\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.493772\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.498834\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.503846\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.508807\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.513719\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.518582\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.503396\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.498362\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.503379\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.498345\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.503362\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.508328\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.503245\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.498212\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.503230\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.508198\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.513116\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.517985\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.512805\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.507677\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.502600\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.507574\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.512498\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.507373\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.512300\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.507177\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.502105\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.507084\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.502013\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.506993\n",
            "resetting env. episode 460.000000, reward total was -18.000000. running mean: -20.481923\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.487104\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.492233\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.497310\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.492337\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.487414\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.492540\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.497614\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.502638\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.507612\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.512536\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.507410\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.512336\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.497213\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.482241\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.487418\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.482544\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.467719\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.473041\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.468311\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.463628\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.468992\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.474302\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.459559\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.464963\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.470313\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.475610\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.470854\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.476146\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.471384\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.476670\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.481904\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.477085\n",
            "resetting env. episode 493.000000, reward total was -18.000000. running mean: -20.452314\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.457791\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.463213\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.458581\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.463995\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.459355\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.464761\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.450114\n",
            "CPU times: user 34min 55s, sys: 12min 56s, total: 47min 51s\n",
            "Wall time: 24min 57s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "98cb261a-f8c2-4271-b66c-64702f5440cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "a0921192-106c-4586-f517-f14a5a2284ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -18.030000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -18.049700\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -18.079203\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -18.098411\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -18.127427\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -18.146153\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -18.174691\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -18.192944\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -18.211015\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.238905\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -18.236516\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -18.264150\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -18.291509\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -18.288594\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -18.315708\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -18.342551\n",
            "resetting env. episode 18.000000, reward total was -18.000000. running mean: -18.339125\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -18.365734\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -18.392077\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -18.418156\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -18.443974\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -18.459535\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -18.484939\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -18.510090\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -18.524989\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -18.549739\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -18.574242\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -18.588499\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -18.612614\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -18.626488\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -18.640223\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -18.653821\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -18.677283\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -18.700510\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -18.723505\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -18.746270\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -18.768807\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -18.791119\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -18.793208\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -18.815276\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -18.827123\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -18.848852\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -18.870363\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -18.881660\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -18.892843\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -18.913915\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -18.924775\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -18.935528\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -18.936172\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.956811\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -18.957243\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -18.957670\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.978093\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -18.998313\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.008329\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.028246\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.047964\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.067484\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.086809\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.095941\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.114982\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.133832\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.152494\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -19.150969\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.169459\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.187764\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.195887\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.213928\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.231789\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.239471\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.257076\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.264505\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.271860\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.289142\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.296250\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.313288\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -19.300155\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.317153\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.333982\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.350642\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -19.347135\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.363664\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.380027\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.396227\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.412265\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.428142\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.443861\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -19.439422\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -19.435028\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.450678\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -19.446171\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.461709\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.457092\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.472521\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -19.467796\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.483118\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.488287\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.503404\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.518370\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.523186\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.527954\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.532675\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.547348\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.561875\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.576256\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.590493\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.584588\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.588742\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.602855\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.616827\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.630658\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.644352\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.657908\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.661329\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.664716\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.678069\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.691288\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.704375\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.707331\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.710258\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.713155\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.716024\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.728864\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.741575\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.744159\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.756718\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.769150\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.771459\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.783744\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.795907\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -19.797948\n",
            "resetting env. episode 133.000000, reward total was -18.000000. running mean: -19.779968\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.792169\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -19.794247\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.806305\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.818241\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.830059\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.841758\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.853341\n",
            "resetting env. episode 141.000000, reward total was -18.000000. running mean: -19.834807\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.846459\n",
            "resetting env. episode 143.000000, reward total was -18.000000. running mean: -19.827995\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.839715\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.851318\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -19.852805\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.854277\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.865734\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.877076\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.888306\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -19.889423\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.900528\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.901523\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.902508\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.913483\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -19.914348\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.925204\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.935952\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.936593\n",
            "resetting env. episode 160.000000, reward total was -16.000000. running mean: -19.897227\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -19.898255\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.909272\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.920179\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.920978\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.931768\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -19.942450\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.953026\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.963495\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -19.953860\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.964322\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -19.964679\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.965032\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -19.975382\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.985628\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -19.975771\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -19.966014\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.976354\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.976590\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -19.986824\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -19.976956\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -19.987186\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.997314\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.007341\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.017268\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.027095\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.036824\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.036456\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.046091\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.055631\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.055074\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.054524\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.063978\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.063339\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.072705\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.081978\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.071158\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.070447\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.079742\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.078945\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.088155\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.087274\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.096401\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.095437\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.104483\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.113438\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.122303\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.131080\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.119770\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.128572\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.127286\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.136013\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.144653\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.153207\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.151675\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.140158\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.148756\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.157269\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.155696\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.164139\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.162498\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.160873\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.149264\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.157771\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.166194\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.164532\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.152886\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.141358\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.129944\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.138645\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.147258\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.155785\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.164228\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.172585\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.180860\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.189051\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.197160\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.205189\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.213137\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.221006\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.218795\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.226608\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.234341\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.241998\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.249578\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.257082\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.244511\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.252066\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.239546\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.247150\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.254679\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.262132\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.259511\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.256916\n",
            "resetting env. episode 254.000000, reward total was -18.000000. running mean: -20.234346\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.222003\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.229783\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.237485\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.245110\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.252659\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.260132\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.267531\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.274856\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.272107\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.279386\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.286592\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.273726\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.270989\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.278279\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.275496\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.282742\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.289914\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.277015\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.274245\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.271502\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.268787\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.276099\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.273338\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.270605\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.277899\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.285120\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.272269\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.269546\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.276851\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.284082\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.291241\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.288329\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.295446\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.282491\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.289666\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.296770\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.293802\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.300864\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.307855\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.304777\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.311729\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.318612\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.315426\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.292271\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.279349\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.276555\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.273790\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.281052\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.288241\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.295359\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.292405\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.299481\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.306486\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.313421\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.320287\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.317084\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.323913\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.320674\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.317468\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.304293\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.311250\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.318138\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.324956\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.331707\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.338390\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.335006\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.341656\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.348239\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.354757\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.361209\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.367597\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.363921\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.370282\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.376579\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.372813\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.379085\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.375294\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.381541\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.387726\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.393849\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.399910\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.405911\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.391852\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.387933\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.394054\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.400113\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.386112\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.372251\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.378529\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.384743\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.380896\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.387087\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.393216\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.389284\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.395391\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.401437\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.397423\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.383449\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.389614\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.385718\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.391861\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.397942\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.403963\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.409923\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.415824\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.421666\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.427449\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.433175\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.438843\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.434454\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.430110\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.425809\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.431551\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.437235\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.422863\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.418634\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.424448\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.430203\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.435901\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.441542\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.447127\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.422656\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.428429\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.434145\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.429803\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.435505\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.441150\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.436739\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.442371\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.437948\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.443568\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.439132\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.444741\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.450294\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.435791\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.441433\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.437019\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.442648\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.448222\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.433740\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.439402\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.435008\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.420658\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.426452\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.422187\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.417965\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.423786\n",
            "resetting env. episode 402.000000, reward total was -18.000000. running mean: -20.399548\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.405552\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.411497\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.417382\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.403208\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.399176\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.405184\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.411132\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.417021\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.392851\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.388922\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.395033\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.391083\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.397172\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.403200\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.399168\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.405176\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.411125\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.417013\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.422843\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.408615\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.414529\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.420383\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.416180\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.422018\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.427798\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.433520\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.439184\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -20.414793\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.420645\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.426438\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.422174\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.407952\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.393873\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.399934\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.395935\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.401975\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.407955\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.413876\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.399737\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.405740\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.401682\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.397665\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.393689\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.399752\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.405754\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.391697\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.397780\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.403802\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.409764\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.415666\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.411510\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.417395\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.413221\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.419089\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.424898\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.430649\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.426342\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.432079\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.437758\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.443380\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.448947\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.454457\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.459913\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.465313\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.460660\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.456054\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.461493\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.456878\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.462309\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.467686\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.473009\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.478279\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.483497\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.488662\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.493775\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.488837\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.483949\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.469109\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.474418\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.479674\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.484877\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.480029\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.465228\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.470576\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.475870\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.461112\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.456500\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.451935\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.457416\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -20.432842\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.438514\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.444128\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.449687\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.455190\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.460638\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.466032\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.471372\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.476658\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.481891\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.487072\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.492202\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.497280\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.492307\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.497384\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.492410\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.497486\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.502511\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.507486\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.512411\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.517287\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.522114\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -20.506893\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.501824\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.496806\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.501838\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.506819\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.501751\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.506734\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.501666\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.506650\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.511583\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.506467\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.511403\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.516289\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.521126\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.515914\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.520755\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.525548\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.520292\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -20.505089\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.510038\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.514938\n",
            "resetting env. episode 535.000000, reward total was -19.000000. running mean: -20.499789\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.504791\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.509743\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.514645\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.509499\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.514404\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.519260\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.524067\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.528827\n",
            "resetting env. episode 544.000000, reward total was -19.000000. running mean: -20.513538\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.498403\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.503419\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.508385\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.493301\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.498368\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.503384\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.498350\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.503367\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.508333\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.503250\n",
            "resetting env. episode 555.000000, reward total was -19.000000. running mean: -20.488217\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.483335\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.468502\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.473817\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -20.459079\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.464488\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.469843\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.475145\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -20.460393\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.445789\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.451331\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.456818\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.462250\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.467627\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.472951\n",
            "resetting env. episode 570.000000, reward total was -19.000000. running mean: -20.458222\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.453639\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.459103\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.454512\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.459967\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.465367\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.470713\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.466006\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.471346\n",
            "resetting env. episode 579.000000, reward total was -18.000000. running mean: -20.446633\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.452167\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.457645\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.463068\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.448438\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.453953\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.459414\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.454820\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.460271\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -20.455669\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.461112\n",
            "resetting env. episode 590.000000, reward total was -19.000000. running mean: -20.446501\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.452036\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.457516\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.462940\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.468311\n",
            "resetting env. episode 595.000000, reward total was -19.000000. running mean: -20.453628\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.459092\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.464501\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.469856\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.475157\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.480406\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.475602\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.480845\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.476037\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.481277\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.486464\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.481599\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.476783\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.482015\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.477195\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.472423\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.477699\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.482922\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.488093\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.493212\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.498280\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.503297\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.508264\n",
            "resetting env. episode 618.000000, reward total was -19.000000. running mean: -20.493181\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.488250\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.493367\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.498433\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.503449\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.508415\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.513330\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.508197\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.513115\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.507984\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.512904\n",
            "resetting env. episode 629.000000, reward total was -18.000000. running mean: -20.487775\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.492897\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.497968\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -20.472989\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.478259\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.483476\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.478642\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.483855\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.489017\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.494126\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.499185\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.504193\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -20.489151\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.494260\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.499317\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.504324\n",
            "resetting env. episode 645.000000, reward total was -18.000000. running mean: -20.479281\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.474488\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.479743\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.484946\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.480096\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.475295\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.480542\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.475737\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.480980\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.486170\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.491308\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.486395\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.491531\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.496616\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.501650\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.496633\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.491667\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.496750\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.501783\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.506765\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.511697\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.506580\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.511514\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.516399\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.521235\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.526023\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.530763\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.535455\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.540100\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.534699\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.529352\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.534059\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.528718\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -20.513431\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.518297\n",
            "resetting env. episode 680.000000, reward total was -19.000000. running mean: -20.503114\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.508083\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.503002\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.497972\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.502992\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.507962\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.512883\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.517754\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.512576\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.517450\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.522276\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.517053\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.521883\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.526664\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.521397\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.526183\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.520921\n",
            "resetting env. episode 697.000000, reward total was -18.000000. running mean: -20.495712\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.480755\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.485948\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.491088\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.496177\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.501215\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.506203\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.511141\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.506030\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.510969\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.515860\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.520701\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.525494\n",
            "resetting env. episode 710.000000, reward total was -19.000000. running mean: -20.510239\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.515137\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.519985\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.524786\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.529538\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.534242\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.528900\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.533611\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.538275\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.542892\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.537463\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.542089\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.536668\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.541301\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.545888\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.550429\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.554925\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.559376\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.553782\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.558244\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.562662\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.557035\n",
            "resetting env. episode 732.000000, reward total was -19.000000. running mean: -20.541465\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.536050\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.540689\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.545283\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.549830\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.554331\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.558788\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.553200\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -20.537668\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.532292\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.526969\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.531699\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.536382\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.541018\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.545608\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.550152\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.554650\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.559104\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.563513\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.557878\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.562299\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.566676\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.571009\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.555299\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.549746\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.554249\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.558706\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.563119\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.567488\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.571813\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.576095\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.580334\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.574531\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -20.558785\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.563197\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.567565\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.561890\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.566271\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.570608\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.574902\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.579153\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.583362\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.587528\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.591653\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -20.575736\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.579979\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.584179\n",
            "resetting env. episode 779.000000, reward total was -18.000000. running mean: -20.558337\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.552754\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.557226\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.551654\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.556137\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -20.540576\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.525170\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.519919\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.524719\n",
            "resetting env. episode 788.000000, reward total was -19.000000. running mean: -20.509472\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.514378\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.519234\n",
            "resetting env. episode 791.000000, reward total was -18.000000. running mean: -20.494041\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.499101\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.484110\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.489269\n",
            "resetting env. episode 795.000000, reward total was -19.000000. running mean: -20.474376\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.479632\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.484836\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.489988\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.485088\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.490237\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -20.475335\n",
            "resetting env. episode 802.000000, reward total was -20.000000. running mean: -20.470581\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.475875\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.481117\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.476306\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.481543\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.486727\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.491860\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.496941\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.501972\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.496952\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.501983\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.496963\n",
            "resetting env. episode 814.000000, reward total was -17.000000. running mean: -20.461993\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.467373\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.472699\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.477972\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.473193\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.468461\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.463776\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.469138\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.474447\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.479703\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.474906\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.480156\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.485355\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.480501\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.485696\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.490839\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.495931\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.490972\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.486062\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.491201\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.486289\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.481426\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.476612\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.461846\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.467228\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.472555\n",
            "resetting env. episode 840.000000, reward total was -18.000000. running mean: -20.447830\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.453351\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.458818\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.464230\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -20.449587\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.445092\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.440641\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.446234\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.451772\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.457254\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.452682\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.448155\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.453673\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.459137\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.454545\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.460000\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.465400\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.470746\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.476038\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.481278\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.486465\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -20.471600\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.476884\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.482116\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.487294\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.482422\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.477597\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.482821\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.477993\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -20.463213\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.468581\n",
            "resetting env. episode 871.000000, reward total was -19.000000. running mean: -20.453895\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.459356\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.464763\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.470115\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.465414\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.470760\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.476052\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.471292\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.476579\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.471813\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.477095\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.482324\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.477501\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.482726\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.467898\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.473219\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.478487\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.483702\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.488865\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.493977\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.489037\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -20.474147\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.479405\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.484611\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.479765\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.484967\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.490118\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.495216\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.500264\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.505262\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.510209\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.515107\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -20.499956\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -20.484956\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.490107\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.495206\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.500254\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.505251\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.510199\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.515097\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.519946\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.524746\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.519499\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.524304\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.519061\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.523870\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.528631\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.533345\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.538012\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.542631\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.537205\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.531833\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.536515\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.541150\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.545738\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.550281\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.544778\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.539330\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.543937\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.548498\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.553013\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.557482\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.561908\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.556289\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.560726\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.565118\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -20.549467\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.543973\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.538533\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.543147\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.537716\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.532339\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.517015\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.521845\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.526627\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.531361\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.536047\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.540686\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.535280\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -20.529927\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.534628\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.539281\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.543888\n",
            "resetting env. episode 954.000000, reward total was -17.000000. running mean: -20.508450\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.513365\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.508231\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.513149\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.518018\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.512837\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.507709\n",
            "resetting env. episode 961.000000, reward total was -18.000000. running mean: -20.482632\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.487806\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.492928\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.497998\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.483018\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.488188\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.493306\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.498373\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.493389\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.488456\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.483571\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.488735\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.483848\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.489009\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -20.484119\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.489278\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.494385\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.499442\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.504447\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.509403\n",
            "resetting env. episode 981.000000, reward total was -19.000000. running mean: -20.494309\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.499366\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.504372\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.509328\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.504235\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.489193\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.474301\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.479558\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.484762\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -20.469914\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.475215\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.480463\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.475659\n",
            "resetting env. episode 994.000000, reward total was -18.000000. running mean: -20.450902\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.456393\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -20.441829\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.447411\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -20.432937\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.438607\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.444221\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.439779\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.445381\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.440927\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -20.436518\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.432153\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.437831\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.443453\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.449018\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.454528\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.459983\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.465383\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -20.450729\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.456222\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.461660\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.467043\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.462373\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.467749\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.473072\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.478341\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.483557\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.488722\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.483835\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.488996\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.494106\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.489165\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.494274\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.499331\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.504338\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.509294\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.504201\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -20.499159\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.504168\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -20.489126\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.494235\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.499292\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.504299\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.509256\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.514164\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.519022\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.513832\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.498694\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.503707\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.508670\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.513583\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.518447\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.523263\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.518030\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -20.502850\n",
            "resetting env. episode 1049.000000, reward total was -19.000000. running mean: -20.487821\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.492943\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.498014\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.493034\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.498103\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.503122\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.498091\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.503110\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -20.498079\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.503098\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.508067\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.512986\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.517857\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.522678\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.527451\n",
            "resetting env. episode 1064.000000, reward total was -17.000000. running mean: -20.492177\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -20.487255\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.482382\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.477559\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.482783\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.487955\n",
            "resetting env. episode 1070.000000, reward total was -19.000000. running mean: -20.473076\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.478345\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.483561\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.488726\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.493839\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.498900\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.503911\n",
            "resetting env. episode 1077.000000, reward total was -18.000000. running mean: -20.478872\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.484083\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.489243\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.484350\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.489507\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.494612\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -20.479665\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -20.464869\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.470220\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.475518\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.480763\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.485955\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.491096\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.496185\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -20.491223\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.496310\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.501347\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.496334\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.501371\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.496357\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.501393\n",
            "resetting env. episode 1098.000000, reward total was -18.000000. running mean: -20.476379\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.481616\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.486799\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.491931\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.497012\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.492042\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -20.467122\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.472450\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.477726\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.472949\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.478219\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.483437\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.488603\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.493717\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.498779\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.493792\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.488854\n",
            "resetting env. episode 1115.000000, reward total was -19.000000. running mean: -20.473965\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.479225\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.484433\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.489589\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.494693\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.499746\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.504749\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.509701\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.514604\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.519458\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.524263\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.529021\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.533731\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -20.528393\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.533109\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.517778\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.522601\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.517375\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.522201\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.526979\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.531709\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.536392\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -20.521028\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -20.505818\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.510759\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.515652\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.520495\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.525290\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -20.520038\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.524837\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.529589\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.524293\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.529050\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.533759\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.538422\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.533038\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.537707\n",
            "resetting env. episode 1152.000000, reward total was -19.000000. running mean: -20.522330\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.517107\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.521936\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.526716\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.531449\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.536135\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.540773\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.535366\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.530012\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.514712\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.519565\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.514369\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.509225\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.514133\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.518992\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.523802\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.528564\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.533278\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.537946\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -20.522566\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.527340\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.532067\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.526746\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -20.511479\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.506364\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.501300\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.506287\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.511225\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.516112\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.520951\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.515742\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.520584\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.525378\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.520125\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.524923\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.509674\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.514577\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.519432\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.514237\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.519095\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.523904\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.528665\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.523378\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.528145\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.522863\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.527634\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.522358\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.527135\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.521863\n",
            "resetting env. episode 1201.000000, reward total was -18.000000. running mean: -20.496645\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.501678\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.496661\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.501695\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.506678\n",
            "resetting env. episode 1206.000000, reward total was -18.000000. running mean: -20.481611\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -20.476795\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.462027\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.467407\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.472733\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.468005\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -20.453325\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.458792\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.464204\n",
            "resetting env. episode 1215.000000, reward total was -17.000000. running mean: -20.429562\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.435266\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.440914\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.446505\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.452040\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.457519\n",
            "resetting env. episode 1221.000000, reward total was -19.000000. running mean: -20.442944\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.448515\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.454029\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.459489\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.454894\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.450345\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.455842\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.461283\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.466671\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.472004\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.477284\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -20.462511\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.457886\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.463307\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.468674\n",
            "resetting env. episode 1236.000000, reward total was -18.000000. running mean: -20.443987\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.449547\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.445052\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.450601\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.456095\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.461534\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.466919\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.472250\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.467527\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.472852\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.478124\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.473342\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.478609\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.483823\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.478985\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.484195\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.489353\n",
            "resetting env. episode 1253.000000, reward total was -18.000000. running mean: -20.464459\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.469815\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.475116\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.470365\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.475662\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.480905\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.486096\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.491235\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -20.486323\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.491459\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -20.476545\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.481779\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.486962\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.492092\n",
            "resetting env. episode 1267.000000, reward total was -19.000000. running mean: -20.477171\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.472399\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.467675\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.472999\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.478269\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.483486\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.488651\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.483765\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -20.468927\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.474238\n",
            "resetting env. episode 1277.000000, reward total was -16.000000. running mean: -20.429495\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.435200\n",
            "resetting env. episode 1279.000000, reward total was -17.000000. running mean: -20.400848\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.396840\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.402871\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.408843\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -20.394754\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.390807\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -20.376899\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.383130\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.389298\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.395405\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.401451\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.407437\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.413363\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.419229\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -20.405037\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -20.390986\n",
            "resetting env. episode 1295.000000, reward total was -19.000000. running mean: -20.377076\n",
            "resetting env. episode 1296.000000, reward total was -19.000000. running mean: -20.363306\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.359673\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.366076\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -20.362415\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.368791\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -20.355103\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.351552\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.358036\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.364456\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.370812\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -20.357103\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.353532\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.359997\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.356397\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.362833\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.369205\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.365513\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.371858\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -20.348139\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.344658\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.351211\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.347699\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -20.334222\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.340880\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.347471\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.343996\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.340556\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.347151\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.353679\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.360142\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.356541\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.362976\n",
            "resetting env. episode 1328.000000, reward total was -19.000000. running mean: -20.349346\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.355852\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.362294\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.368671\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.364984\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.361334\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.367721\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.374044\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.370303\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.376600\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.382834\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.389006\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -20.385116\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.381265\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.387452\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.393578\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.399642\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.405645\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.401589\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -20.387573\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.383697\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.379860\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.386062\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.382201\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.378379\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.384595\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.380749\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.366942\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.373272\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.369540\n",
            "resetting env. episode 1358.000000, reward total was -18.000000. running mean: -20.345844\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.352386\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.358862\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.365273\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.361621\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.358004\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.364424\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.370780\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.377072\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.363302\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.369669\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -20.365972\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.372312\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.378589\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.384803\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.390955\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.397046\n",
            "resetting env. episode 1375.000000, reward total was -18.000000. running mean: -20.373075\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.379344\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.375551\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -20.371795\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.378078\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.364297\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -20.370654\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.376947\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.383178\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.389346\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.395453\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.391498\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.397583\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.403607\n",
            "resetting env. episode 1389.000000, reward total was -19.000000. running mean: -20.389571\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.395675\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.391719\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.397801\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.403823\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.409785\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.415687\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.421530\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.427315\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -20.413042\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.418912\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.424723\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.430475\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.436171\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.441809\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.447391\n",
            "resetting env. episode 1405.000000, reward total was -19.000000. running mean: -20.432917\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.438588\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -20.414202\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.420060\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.425859\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.431601\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.437285\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.442912\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.448483\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.443998\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -20.439558\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.445162\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.450711\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -20.436203\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.441841\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.437423\n",
            "resetting env. episode 1421.000000, reward total was -17.000000. running mean: -20.403049\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.409018\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.414928\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.420779\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.426571\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.432305\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.437982\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.443602\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.449166\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.454675\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.460128\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.445527\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.451072\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.456561\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.461995\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.457375\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.462801\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.468173\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.473492\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -20.458757\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.464169\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.469528\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.474832\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.480084\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.475283\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -20.470530\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.475825\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.461067\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.456456\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.461891\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.467273\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.472600\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.477874\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.483095\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.488264\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.493382\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.498448\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.503463\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.498429\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.503444\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.508410\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -20.493326\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.498393\n",
            "resetting env. episode 1464.000000, reward total was -19.000000. running mean: -20.483409\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.488575\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.493689\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.498752\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.503764\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.488727\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.493839\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.488901\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.484012\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -20.469172\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.464480\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.469835\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.475137\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -20.470386\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.475682\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.480925\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.476116\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.481355\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.486541\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -20.471676\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.476959\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.482189\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.487367\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -20.482494\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.487669\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.492792\n",
            "resetting env. episode 1490.000000, reward total was -17.000000. running mean: -20.457864\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.463286\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.468653\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -20.443966\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.449526\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.455031\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.450481\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.455976\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.461416\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.466802\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.472134\n",
            "CPU times: user 1h 33min 30s, sys: 36min 8s, total: 2h 9min 38s\n",
            "Wall time: 1h 7min 18s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "0ad4af3e-bc9e-41e8-8726-95724c9c9d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}