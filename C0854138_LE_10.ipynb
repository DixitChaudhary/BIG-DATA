{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "e8d491bc-ae60-412b-f6ac-d773cfb32540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 27.0 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=12562455eca37e1ebc852f16dfb58ad97c27255cdb91182391453c9b0158721e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "8a88da49-31b1-42da-e27f-9fbef923f140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "d0cf0a5a-6bc2-4569-d93d-12cd56dcfaf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "77b76e72-7466-4f0b-b818-4c8201c420c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "e60157e5-51b9-4433-a0e7-16b27ed9f7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-10\n",
        "learning_rate = 1e-10\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "7c5706ca-5103-47ba-a611-3118de016734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -20.970199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.970497\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970792\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.961084\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.951473\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951959\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.952439\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.952915\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.953385\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.943852\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.944413\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.944969\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.945519\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.946064\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.936603\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.927237\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.917965\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.918785\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.909597\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.910502\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.911397\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.892283\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.883360\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.884526\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.865681\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.867024\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.868354\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.859670\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.861074\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.852463\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.843938\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.825499\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.817244\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.819071\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.810881\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.802772\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.794744\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.786797\n",
            "resetting env. episode 42.000000, reward total was -18.000000. running mean: -20.758929\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.761339\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.763726\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.766089\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.758428\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.760844\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.763235\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.755603\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.758047\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.750466\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.742962\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.735532\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.728177\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.720895\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.723686\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.716449\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.719285\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.712092\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.704971\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.697921\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.700942\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.703933\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.706893\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.709824\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.712726\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.715599\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.708443\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.711358\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.714245\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.707102\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.710031\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.702931\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.705902\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.708843\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.711754\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.694637\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.697690\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.700713\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.683706\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.686869\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.690001\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.693101\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.686170\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.689308\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.692415\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.685491\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.688636\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.691749\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.694832\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.687884\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.671005\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.654295\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.647752\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.641274\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.644861\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.628413\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.632129\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.635807\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.639449\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.643055\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.646624\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.640158\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.623756\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.627519\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.611244\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.615131\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.598980\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.602990\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.606960\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.610891\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.604782\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.598734\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.602747\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.596719\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.600752\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.604744\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.608697\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.612610\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.606484\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.610419\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.614315\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.618172\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.611990\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.615870\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.619711\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.613514\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.617379\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.601205\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.605193\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.599141\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.603150\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.587118\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.591247\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.585335\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.589481\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.593587\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.597651\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.591674\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.595758\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.599800\n",
            "resetting env. episode 142.000000, reward total was -18.000000. running mean: -20.573802\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.568064\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.572383\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.576659\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.570893\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.565184\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.569532\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.563837\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.548198\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.532716\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.517389\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.522215\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.526993\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.531723\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.526406\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.531142\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.525831\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.530572\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.535267\n",
            "resetting env. episode 161.000000, reward total was -17.000000. running mean: -20.499914\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.504915\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.499866\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.504867\n",
            "resetting env. episode 165.000000, reward total was -17.000000. running mean: -20.469818\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.475120\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.470369\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.475665\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.480909\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.476099\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.481338\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.476525\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.461760\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.447142\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.452671\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.458144\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.463563\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.468927\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.464238\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.439595\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.435199\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.430847\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.436539\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.442174\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.447752\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.443274\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.448842\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.434353\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.440010\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.445610\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.451153\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.456642\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.462075\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.457455\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.462880\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.468251\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.473569\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.458833\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.454245\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.439702\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.445305\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.450852\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.456344\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.461780\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.467163\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.462491\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.467866\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.473187\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.478455\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.483671\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.488834\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.493946\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.499006\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.504016\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.498976\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.503986\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.508947\n",
            "resetting env. episode 218.000000, reward total was -18.000000. running mean: -20.483857\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.479019\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.484228\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.479386\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.474592\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.479846\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.475048\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.480297\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.485494\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.490639\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.495733\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.500776\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.505768\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.510710\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.515603\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.520447\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.525243\n",
            "resetting env. episode 235.000000, reward total was -16.000000. running mean: -20.479990\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.465190\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.460538\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.455933\n",
            "resetting env. episode 239.000000, reward total was -18.000000. running mean: -20.431374\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.427060\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.432789\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.438461\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.434077\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.429736\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.435439\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.441084\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.436674\n",
            "resetting env. episode 248.000000, reward total was -18.000000. running mean: -20.412307\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.408184\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.414102\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.419961\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.425761\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.431504\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.437189\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.442817\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.438389\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.444005\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.439565\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.435169\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.440817\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.446409\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.451945\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.457426\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.462851\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.468223\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.473541\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.478805\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.484017\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.479177\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.484385\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.479541\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.484746\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.489898\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.494999\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -20.470049\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.475349\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.470595\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.475890\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.481131\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.486319\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.491456\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.496542\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.501576\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.506560\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.501495\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.506480\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.501415\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.496401\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.501437\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.506423\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.511358\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.496245\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.501282\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.506269\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.511207\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.516095\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.520934\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.515724\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.500567\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.495561\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.500606\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.505600\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.510544\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.515438\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.510284\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.505181\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.510129\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.515028\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -20.489878\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.494979\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.480029\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.475229\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.480477\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.485672\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.490815\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.495907\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.500948\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.495938\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.500979\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.475969\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.481210\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -20.456397\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.441833\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.437415\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.443041\n",
            "resetting env. episode 326.000000, reward total was -17.000000. running mean: -20.408611\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.414524\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.420379\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.416175\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.422014\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.407794\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.413716\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.419578\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.425383\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.431129\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.426818\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.432549\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.428224\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.423942\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.419702\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.425505\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.431250\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.436938\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.432568\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.438243\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.443860\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.449422\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.454927\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.450378\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.455874\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.441316\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.446902\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.452433\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.457909\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.463330\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.458697\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.444110\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.449669\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.455172\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.460620\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.446014\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.451554\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.457038\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.462468\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.457843\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.443265\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.448832\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.454344\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.459800\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.465202\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.470550\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.465845\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.471186\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.476475\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.481710\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.486893\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.472024\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.467304\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.472631\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.477904\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.473125\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.478394\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.473610\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.468874\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.464185\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.469543\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.464848\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.470199\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.475497\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.480742\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.485935\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.481076\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.486265\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.481402\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.466588\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.471922\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.477203\n",
            "resetting env. episode 398.000000, reward total was -18.000000. running mean: -20.452431\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.457907\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.453328\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.458794\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.454206\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.459664\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.465068\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.470417\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.475713\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.480956\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.466146\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.471485\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.476770\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.482002\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.487182\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.482310\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.477487\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.472712\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.467985\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.473305\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.478572\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.473787\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.479049\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.474258\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.479516\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.484721\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.489873\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.494975\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.500025\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.505025\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.509974\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.514875\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.519726\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.524529\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.529283\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.523991\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.528751\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.523463\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.528228\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.532946\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.537617\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.522241\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.527018\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.521748\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.526530\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.531265\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.535953\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.530593\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.535287\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.529934\n",
            "resetting env. episode 448.000000, reward total was -18.000000. running mean: -20.504635\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.499589\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.494593\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.499647\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.504650\n",
            "resetting env. episode 453.000000, reward total was -16.000000. running mean: -20.459604\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.465008\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.470358\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.465654\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.470997\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.476288\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.471525\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.456809\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.462241\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.457619\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.463043\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.458412\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.453828\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.459290\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.464697\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.470050\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.475350\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.480596\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.485790\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.490932\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.496023\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.501063\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.506052\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.510991\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.515882\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.510723\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.505615\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.510559\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.515454\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.520299\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.525096\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.529845\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.534547\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.509201\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.494109\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.499168\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.504177\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.499135\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.494143\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.499202\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.494210\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.499268\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.504275\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.509232\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.514140\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.518999\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.513809\n",
            "resetting env. episode 500.000000, reward total was -17.000000. running mean: -20.478671\n",
            "CPU times: user 22min 29s, sys: 10min 18s, total: 32min 48s\n",
            "Wall time: 17min\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "d428cd27-cfba-4596-ac99-648339c55af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.039010\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.048620\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.058134\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.057552\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -20.036977\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.036607\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.046241\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.045779\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.035321\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.044968\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.044518\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.054073\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.063532\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.072897\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.082168\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.081346\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.090533\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.099627\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.108631\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.097545\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.086569\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.095704\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.094746\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.103799\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.112761\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.111633\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.090517\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.099612\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.108616\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.117530\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.106354\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -20.085291\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.094438\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.093493\n",
            "resetting env. episode 40.000000, reward total was -16.000000. running mean: -20.052559\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.062033\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.061413\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.070799\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.080091\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.089290\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.088397\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.087513\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.096638\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -20.075671\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.074915\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.084165\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.083324\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.092491\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.101566\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.110550\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.099444\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.098450\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.107466\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.116391\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.115227\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.124075\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.102834\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.111806\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.120688\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.129481\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.128186\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.136904\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.135535\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.124180\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.132938\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.131608\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.140292\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.148889\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.157401\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.155827\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.164268\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.172626\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.180899\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.189090\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.197199\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.185227\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.193375\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.201441\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.189427\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.197533\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.195557\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.203602\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.201566\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.209550\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.217455\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.225280\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.233027\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.230697\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.238390\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.236006\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.223646\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.231410\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.229096\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.236805\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.244437\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.251992\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -20.229472\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.217178\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.205006\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.212956\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.220826\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.228618\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.236332\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.243968\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.251529\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.249013\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.256523\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.263958\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.261318\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.258705\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.266118\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.273457\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.280722\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.287915\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.275036\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.282286\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.289463\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.296568\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.303603\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.310567\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.317461\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.324286\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.311043\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.317933\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.304754\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.311706\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.318589\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.325403\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.332149\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.338828\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.345439\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.341985\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.338565\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.325179\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.321928\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.328708\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.335421\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.342067\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.348646\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.345160\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.351708\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.358191\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.364609\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.360963\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.347354\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.343880\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.340441\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.347037\n",
            "resetting env. episode 154.000000, reward total was -17.000000. running mean: -20.313567\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.310431\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.307327\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.314253\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.321111\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.317900\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.324721\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.311473\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.318359\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.315175\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.322023\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.328803\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.335515\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.342160\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.348738\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.355251\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.361698\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.368081\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.374401\n",
            "resetting env. episode 173.000000, reward total was -18.000000. running mean: -20.350657\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.357150\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.353579\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.360043\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.366442\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.362778\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.369150\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.375459\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.371704\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.367987\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.374307\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.360564\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.346958\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.353489\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.359954\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.366354\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.372691\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.378964\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.375174\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.381423\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.387608\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.393732\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.399795\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.405797\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.411739\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.417622\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.423445\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.429211\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.434919\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.440570\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.426164\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.431902\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.437583\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.443207\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.448775\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.434288\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.439945\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.425545\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.431290\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.436977\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.442607\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.448181\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.443699\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.429262\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.434970\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.440620\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.446214\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.451752\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.457234\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.452662\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.458135\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.453554\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.459018\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.454428\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.449884\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.435385\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.441031\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.446621\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.452155\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.457633\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.453057\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.458526\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.443941\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.429502\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.435207\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.440854\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.446446\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.451981\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.437462\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.433087\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.428756\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.434469\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.430124\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.435823\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.441464\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.447050\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.452579\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.458053\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.463473\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.458838\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.454250\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.459707\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.465110\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.470459\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.475755\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.470997\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.466287\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.461624\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.467008\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.472338\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.477614\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.462838\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.468210\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.463528\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.468893\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.474204\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.469462\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.474767\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.480019\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.485219\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.490367\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.485463\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.490609\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.495703\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.500746\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.505738\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.500681\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.505674\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.500617\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.505611\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.510555\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.515449\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.520295\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.525092\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.529841\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.524543\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.509297\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.514204\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.499062\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.494071\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.479131\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.484339\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.489496\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.484601\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.489755\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.494858\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.499909\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.504910\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.509861\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.514762\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.499615\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.504618\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.509572\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.494477\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.489532\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.494636\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.489690\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.494793\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.499845\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.504847\n",
            "resetting env. episode 313.000000, reward total was -17.000000. running mean: -20.469798\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.455100\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.460549\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.465944\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.461284\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.446672\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.442205\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.447783\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.453305\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.458772\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.464184\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.469542\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.474847\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.470098\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.465397\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.470743\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.466036\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.471376\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.466662\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.461995\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.457375\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.462802\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.468174\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.473492\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.468757\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.474069\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.479329\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.484535\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.489690\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.484793\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.479945\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.485146\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.490294\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.495391\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.500437\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.505433\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.510379\n",
            "resetting env. episode 350.000000, reward total was -18.000000. running mean: -20.485275\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.480422\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.485618\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.480762\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.485954\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.491095\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.486184\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.481322\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.486509\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.481644\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.476827\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.482059\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.487238\n",
            "resetting env. episode 363.000000, reward total was -17.000000. running mean: -20.452366\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.447842\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.453364\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.458830\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.454242\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.439699\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.435302\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.440949\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.446540\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.442075\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.437654\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.433277\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.428944\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.434655\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.440308\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.445905\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.451446\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.446932\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.442463\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.428038\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.413758\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.409620\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.415524\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.421369\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.427155\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.422883\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.418654\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.424468\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.430223\n",
            "resetting env. episode 392.000000, reward total was -18.000000. running mean: -20.405921\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.411862\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.407743\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.403666\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.399629\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.405633\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.391576\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.387661\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.383784\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.379946\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.376147\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.372385\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.368661\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.374975\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.361225\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.357613\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.364037\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.370396\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.376692\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.382925\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.379096\n",
            "resetting env. episode 413.000000, reward total was -17.000000. running mean: -20.345305\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.331852\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.328534\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.335248\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.341896\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.348477\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.354992\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.361442\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.367828\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.354150\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.350608\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.347102\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.353631\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.350095\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.346594\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.353128\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.359596\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.366000\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.362340\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.368717\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.375030\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.381280\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.387467\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.393592\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.389656\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.395760\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.391802\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.397884\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.403905\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.409866\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.415767\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.421610\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.427394\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.433120\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.438789\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.444401\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.449957\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.435457\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.441103\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.446692\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.452225\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.437702\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.433325\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.438992\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.444602\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.440156\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.425755\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.421497\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.427282\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.423009\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.418779\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.424591\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.410345\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.416242\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.422080\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.427859\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.433580\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.429244\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.434952\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.440602\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.446196\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.431734\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.437417\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.433043\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.428712\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.434425\n",
            "resetting env. episode 479.000000, reward total was -18.000000. running mean: -20.410081\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.395980\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.392020\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.378100\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.384319\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.380476\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.386671\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.392805\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.398877\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.404888\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.410839\n",
            "resetting env. episode 490.000000, reward total was -17.000000. running mean: -20.376731\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.372963\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.379234\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.375441\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.381687\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.387870\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.393991\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.390051\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.386151\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.392289\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.398366\n",
            "CPU times: user 22min 22s, sys: 10min 14s, total: 32min 37s\n",
            "Wall time: 16min 49s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "6d8d1e95-fe65-45c7-ab9f-ee6059a6c8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "80d25c60-5cb4-40f4-e145-758026d1a442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -18.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -18.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -18.059601\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -18.089005\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -18.118115\n",
            "resetting env. episode 7.000000, reward total was -18.000000. running mean: -18.116934\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -18.135764\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -18.154407\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -18.182863\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.211034\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -18.208924\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -18.236835\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -18.264466\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -18.291822\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -18.318903\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -18.335714\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -18.362357\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -18.388734\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -18.394846\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -18.420898\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -18.446689\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -18.472222\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -18.497500\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -18.522525\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -18.547299\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -18.571826\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -18.596108\n",
            "resetting env. episode 29.000000, reward total was -18.000000. running mean: -18.590147\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -18.614246\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -18.638103\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -18.651722\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -18.675205\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -18.698453\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -18.721468\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -18.744254\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -18.746811\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -18.769343\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -18.791650\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -18.813733\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -18.825596\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -18.837340\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -18.838966\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -18.850577\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -18.862071\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -18.873450\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -18.894716\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -18.915769\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -18.916611\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -18.937445\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.958070\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -18.948490\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -18.959005\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.979415\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -18.969621\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -18.989924\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.000025\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.010025\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.029925\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.049625\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.059129\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.078538\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.097752\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.116775\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.125607\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.144351\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.162908\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.181279\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.189466\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -19.187571\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.205695\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.213638\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.211502\n",
            "resetting env. episode 74.000000, reward total was -18.000000. running mean: -19.199387\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.217393\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.235219\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.252867\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.270338\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.287635\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.294759\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.311811\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.328693\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.345406\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -19.351952\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -19.348432\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.354948\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -19.351399\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.367885\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -19.364206\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.380564\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.386758\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.402890\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.418862\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.434673\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.450326\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -19.445823\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.451365\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.466851\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.482183\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.487361\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.492487\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.497562\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.502587\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -19.497561\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.512585\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -19.517459\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.532285\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.536962\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -19.531592\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.546276\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.560814\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.565205\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.579553\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.583758\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.597920\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.601941\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.615922\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.629762\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.643465\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.647030\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.660560\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -19.673954\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.687215\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.690343\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.693439\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.706505\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.699440\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.702445\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.715421\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.728267\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.730984\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.743674\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.756237\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.768675\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.780988\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -19.773178\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -19.775447\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.787692\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.799815\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.811817\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.823699\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -19.815462\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.827307\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.839034\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.850644\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.862137\n",
            "resetting env. episode 147.000000, reward total was -18.000000. running mean: -19.843516\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.855081\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.866530\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.877865\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -19.879086\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -19.880295\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.891492\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.902577\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.913552\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.924416\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.935172\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -19.925820\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -19.936562\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -19.937196\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.947824\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.958346\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.958763\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.969175\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.979483\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -19.989689\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.999792\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.999794\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.009796\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.019698\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.029501\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.039206\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.038814\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.048426\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.057941\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.067362\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.076688\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.085921\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.095062\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.084112\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.093271\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.102338\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.111314\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.110201\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.119099\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.127908\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.136629\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.135263\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.143910\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.132471\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.141146\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.139735\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.138338\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.146954\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.155485\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.163930\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.172291\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.170568\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.178862\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.187073\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.175203\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.173451\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.171716\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.159999\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.168399\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.166715\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.175048\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.183297\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.191464\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.189550\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.177654\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.185878\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.194019\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.182079\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.190258\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.188355\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.166472\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.164807\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.173159\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.171427\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.179713\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.167916\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.176237\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.184475\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.192630\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.200703\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.198696\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.196709\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.204742\n",
            "resetting env. episode 230.000000, reward total was -17.000000. running mean: -20.172695\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.180968\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.169158\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.177467\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.185692\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.193835\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.191897\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.189978\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.188078\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.196197\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.204235\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.202193\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.200171\n",
            "resetting env. episode 243.000000, reward total was -18.000000. running mean: -20.178169\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.186388\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.194524\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.202578\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.210553\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.198447\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.206463\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.214398\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.222254\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.230032\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.237731\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.245354\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.252900\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.260371\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.267768\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.265090\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.262439\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.249815\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.257317\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.264743\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.272096\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.269375\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.276681\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.283914\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.291075\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.298165\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.305183\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.292131\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.299210\n",
            "resetting env. episode 272.000000, reward total was -17.000000. running mean: -20.266218\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.273555\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.280820\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.288012\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.295132\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.292180\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.299258\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.306266\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.313203\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.320071\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.326871\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.333602\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.340266\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.336863\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.323494\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.330260\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.336957\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.343587\n",
            "resetting env. episode 290.000000, reward total was -17.000000. running mean: -20.310152\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.317050\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.313879\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.310741\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.317633\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.324457\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.331212\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.307900\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.304821\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.311773\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.298655\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.285669\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.292812\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.299884\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.296885\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.303916\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.300877\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.297868\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.294890\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.301941\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.308921\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.305832\n",
            "resetting env. episode 312.000000, reward total was -18.000000. running mean: -20.282774\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.289946\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.297047\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.304076\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.311035\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.307925\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.304846\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.311797\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.308679\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.315593\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.322437\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.319212\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.326020\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.332760\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.339432\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.346038\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.352578\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.339052\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.345661\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.352205\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.358683\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.365096\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.371445\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.367730\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.374053\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.380313\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.386509\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.392644\n",
            "resetting env. episode 340.000000, reward total was -17.000000. running mean: -20.358718\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.365131\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.371479\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.367765\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.354087\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.360546\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.366941\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.373271\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.379539\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.385743\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.391886\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.377967\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.374187\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.380445\n",
            "resetting env. episode 354.000000, reward total was -18.000000. running mean: -20.356641\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.363074\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.369444\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.375749\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.381992\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.388172\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.384290\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.390447\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.396543\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.402577\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.398552\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.394566\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.390620\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.396714\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.382747\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.368920\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.375230\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.361478\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.367863\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.354185\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.360643\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.337036\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.323666\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.320429\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.317225\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.304053\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.301012\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.308002\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.314922\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.321773\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.318555\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.315370\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.322216\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.328994\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.325704\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.312447\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.309322\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.316229\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.303067\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.310036\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.316936\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.323766\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.320529\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.327324\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.334050\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.340710\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.327303\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.334030\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.340689\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.347282\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.343810\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.350372\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.356868\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.363299\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.369666\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.365969\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.372310\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.378587\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.384801\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.380953\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.387143\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.393272\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.399339\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.405346\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.411292\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.417179\n",
            "resetting env. episode 420.000000, reward total was -18.000000. running mean: -20.393008\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.399078\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.395087\n",
            "resetting env. episode 423.000000, reward total was -18.000000. running mean: -20.371136\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.377425\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.363650\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.370014\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.356314\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.342750\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.349323\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.345830\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.352371\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.358848\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.365259\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.371607\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.377891\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.384112\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.390271\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.386368\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.392504\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.398579\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.384593\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.380747\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.386940\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.383071\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.389240\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.385347\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.391494\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.377579\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.383803\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.369965\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.366266\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.372603\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.378877\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.375088\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.371337\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.377624\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.383848\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.380009\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.376209\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.382447\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.388622\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.394736\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.390789\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.396881\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.402912\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.398883\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.384894\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.391045\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.397135\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.403163\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.409132\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.415041\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.420890\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.426681\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.432414\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.428090\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.433809\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -20.409471\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.415377\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.421223\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.427011\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.432740\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.428413\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.434129\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.429788\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.435490\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.441135\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.446724\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.452256\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.447734\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.443256\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.428824\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.434536\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.440190\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.445788\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.451330\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.446817\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.452349\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.447825\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.453347\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.458814\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.454226\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.459683\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.455087\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.450536\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.446030\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.451570\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.447054\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.452584\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.458058\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.463477\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.458843\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.454254\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.449712\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.455214\n",
            "resetting env. episode 516.000000, reward total was -18.000000. running mean: -20.430662\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.436356\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.421992\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.427772\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.433495\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.439160\n",
            "resetting env. episode 522.000000, reward total was -18.000000. running mean: -20.414768\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.410620\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.416514\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.412349\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.408225\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.404143\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.410102\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.416001\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.411841\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.417722\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.423545\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.419310\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.425117\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.420865\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.416657\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.422490\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.428265\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.423983\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.429743\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.435445\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.441091\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.446680\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.442213\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.447791\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.453313\n",
            "resetting env. episode 547.000000, reward total was -18.000000. running mean: -20.428780\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.434492\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.440147\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.435746\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.441388\n",
            "resetting env. episode 552.000000, reward total was -18.000000. running mean: -20.416974\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.422805\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.418577\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.424391\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.420147\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.425946\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.431686\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.437369\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.432996\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.438666\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.434279\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.439936\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.445537\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.441081\n",
            "resetting env. episode 566.000000, reward total was -18.000000. running mean: -20.416671\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.422504\n",
            "resetting env. episode 568.000000, reward total was -18.000000. running mean: -20.398279\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.394296\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.400353\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.406350\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.402286\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.408263\n",
            "resetting env. episode 574.000000, reward total was -18.000000. running mean: -20.384181\n",
            "resetting env. episode 575.000000, reward total was -19.000000. running mean: -20.370339\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.376635\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.382869\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.389040\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.395150\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.401198\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.407186\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.403115\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.409083\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -20.394993\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.401043\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.407032\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.402962\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.408932\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.414843\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.420695\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.416488\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.422323\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.428100\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.423819\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.429580\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.425285\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.421032\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.426821\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.432553\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.428228\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.433945\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.429606\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.435310\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.440957\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.446547\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.442082\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.447661\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.443184\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.448752\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.454265\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.449722\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.445225\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.440773\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.436365\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.442001\n",
            "resetting env. episode 616.000000, reward total was -18.000000. running mean: -20.417581\n",
            "resetting env. episode 617.000000, reward total was -18.000000. running mean: -20.393406\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.389472\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.395577\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.401621\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.397605\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.403629\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.399592\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.405597\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.411541\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.417425\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.413251\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.419118\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.424927\n",
            "resetting env. episode 630.000000, reward total was -18.000000. running mean: -20.400678\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.396671\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -20.372704\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.368977\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.375288\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.381535\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.387719\n",
            "resetting env. episode 637.000000, reward total was -19.000000. running mean: -20.373842\n",
            "resetting env. episode 638.000000, reward total was -19.000000. running mean: -20.360104\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.366503\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.362838\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.369209\n",
            "resetting env. episode 642.000000, reward total was -19.000000. running mean: -20.355517\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.361962\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.368342\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.374659\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.380912\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.387103\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.393232\n",
            "resetting env. episode 649.000000, reward total was -18.000000. running mean: -20.369300\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.375607\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.381851\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -20.368032\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.364352\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.350709\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.357201\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.353629\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.360093\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.366492\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.372827\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.369099\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.365408\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.371754\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.368036\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.374356\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.380613\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.376806\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.363038\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.369408\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.375714\n",
            "resetting env. episode 670.000000, reward total was -18.000000. running mean: -20.351957\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.348437\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.344953\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.341503\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.348088\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.354607\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.361061\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -20.347451\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.353976\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.360436\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.366832\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.363164\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.359532\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.365937\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.372277\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.358555\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.364969\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.371319\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.377606\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.383830\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.389992\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.376092\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.382331\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.388508\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.394623\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.400676\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.406670\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.402603\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.398577\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.404591\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.400545\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.406540\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.412474\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.418350\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.424166\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.429924\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.435625\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.431269\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.436956\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.432587\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.438261\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.433878\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.429539\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.425244\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.420992\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.416782\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -20.402614\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.408588\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.404502\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.410457\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.416352\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.422189\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.427967\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.433687\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.439350\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.444957\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.450507\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.456002\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.461442\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.456828\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.462259\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.467637\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.472960\n",
            "resetting env. episode 733.000000, reward total was -16.000000. running mean: -20.428231\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.433949\n",
            "resetting env. episode 735.000000, reward total was -19.000000. running mean: -20.419609\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.415413\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.421259\n",
            "resetting env. episode 738.000000, reward total was -19.000000. running mean: -20.407046\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.412976\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.418846\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.424658\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.430411\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.436107\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.441746\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.447328\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.452855\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.458327\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.463743\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.459106\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.464515\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.449870\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.455371\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.450817\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.446309\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.441846\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.437428\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.433053\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.428723\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.434435\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.430091\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.435790\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.441432\n",
            "resetting env. episode 763.000000, reward total was -17.000000. running mean: -20.407018\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.402948\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.398918\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.404929\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.400880\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.406871\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.412802\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.418674\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -20.404488\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.410443\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.396338\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.402375\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.398351\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.404368\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.410324\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.416221\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.422059\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.427838\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.423560\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.419324\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.425131\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.430879\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -20.426571\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.432305\n",
            "resetting env. episode 787.000000, reward total was -19.000000. running mean: -20.417982\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.423802\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.429564\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.435268\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.440916\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.436507\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.442141\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.447720\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.453243\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.458710\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.464123\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.469482\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.474787\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.480039\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.485239\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.490387\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.485483\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.480628\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.485822\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.480963\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.476154\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.481392\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.486578\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.481713\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.476895\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.472126\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.477405\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.472631\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.477905\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.483126\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.478295\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.483512\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.478676\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.473890\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.479151\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.484359\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.489516\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.484621\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.479774\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.484977\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.480127\n",
            "resetting env. episode 828.000000, reward total was -18.000000. running mean: -20.455326\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.450772\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.446265\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.441802\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.447384\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.442910\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.448481\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.453996\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.459456\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.444862\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.450413\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.455909\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.461350\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.466736\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.452069\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.457548\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.462973\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.468343\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.473660\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.478923\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.484134\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -20.469292\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.474600\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.469854\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.475155\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.480403\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.485599\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.490743\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.495836\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.490878\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.485969\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.481109\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.486298\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.491435\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.486521\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.491656\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.496739\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.501772\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.506754\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.511686\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.516569\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.511404\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.506290\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.501227\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.506215\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.501152\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.506141\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -20.491079\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.486169\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.491307\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.496394\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.501430\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.506416\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.501352\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.506338\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.501275\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.506262\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.511199\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.506087\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.511026\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.515916\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.520757\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.525549\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.530294\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -20.514991\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.519841\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.524643\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.529396\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.534102\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.538761\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.543374\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.537940\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.542561\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.547135\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.541664\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.546247\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.550784\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.555277\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.559724\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.564127\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.568485\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.562800\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.557172\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.561601\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.555985\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.560425\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.564821\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.559172\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.563581\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.567945\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.572265\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.556543\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.560977\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.565368\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -20.549714\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.554217\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.558675\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.553088\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.557557\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.561981\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.566362\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.570698\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.564991\n",
            "resetting env. episode 931.000000, reward total was -19.000000. running mean: -20.549341\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.553848\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.538309\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.542926\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.537497\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.532122\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.526801\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.531533\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.526217\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -20.510955\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.505846\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.510787\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.505679\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.510622\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.495516\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -20.480561\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.475755\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.480998\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.486188\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.491326\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.486413\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.491549\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.486633\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.491767\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.496849\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.501881\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.506862\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.501793\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.496775\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.501808\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.506790\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.501722\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.506704\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.511637\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.516521\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.521356\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.526142\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.530881\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.525572\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.530316\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.525013\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.529763\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.534465\n",
            "resetting env. episode 974.000000, reward total was -18.000000. running mean: -20.509121\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.514029\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -20.498889\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.493900\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.498961\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -20.493972\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.489032\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.494142\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -20.479200\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.484408\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.489564\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.494669\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.499722\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.494725\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.489777\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.494880\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -20.479931\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.485131\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.480280\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.475477\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.480723\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.485915\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.491056\n",
            "resetting env. episode 997.000000, reward total was -18.000000. running mean: -20.466146\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.471484\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.466769\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.472102\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -20.457381\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.462807\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.468179\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.453497\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.458962\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.454372\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.459829\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.465230\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.460578\n",
            "resetting env. episode 1010.000000, reward total was -18.000000. running mean: -20.435972\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.441613\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.447196\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.452724\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.458197\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.453615\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.459079\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.464488\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.459843\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.465245\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.470593\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.465887\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.471228\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.476515\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.481750\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.486933\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.492063\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.497143\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.482171\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.487350\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.492476\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.497551\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.492576\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -20.477650\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.482874\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.478045\n",
            "resetting env. episode 1036.000000, reward total was -18.000000. running mean: -20.453264\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.458732\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.454145\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.459603\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.465007\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.470357\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.475653\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.480897\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.486088\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.481227\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.476415\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.481651\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.486834\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.481966\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.487146\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.482275\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -20.467452\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.462777\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.468150\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.473468\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.468733\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.474046\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.479306\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.484513\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.489667\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.494771\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.499823\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.494825\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.479877\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -20.465078\n",
            "resetting env. episode 1066.000000, reward total was -19.000000. running mean: -20.450427\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.455923\n",
            "resetting env. episode 1068.000000, reward total was -19.000000. running mean: -20.441364\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.446950\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.442480\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.448056\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.443575\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.449139\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.454648\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.460101\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.465500\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.470845\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.476137\n",
            "resetting env. episode 1079.000000, reward total was -19.000000. running mean: -20.461376\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.466762\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.472094\n",
            "resetting env. episode 1082.000000, reward total was -17.000000. running mean: -20.437373\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.443000\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -20.438570\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.444184\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.449742\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.455245\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.460692\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.456085\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.451524\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.457009\n",
            "resetting env. episode 1092.000000, reward total was -17.000000. running mean: -20.422439\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.428215\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.433933\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.439593\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.445197\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.450745\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.456238\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.461675\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.467059\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.472388\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.477664\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.482888\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.488059\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.493178\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.498246\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -20.483264\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.478431\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.483647\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.478810\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.484022\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.479182\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.484390\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.489546\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.494651\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.499704\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.504707\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.509660\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.504564\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.509518\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.494423\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.499479\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -20.484484\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.489639\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.494743\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.489795\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.494897\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.499948\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -20.494949\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.489999\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.485099\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.490248\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.495346\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.500392\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.505388\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.500335\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -20.485331\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.490478\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.495573\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.500617\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -20.485611\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.470755\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.476048\n",
            "resetting env. episode 1144.000000, reward total was -18.000000. running mean: -20.451287\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.436774\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.442406\n",
            "resetting env. episode 1147.000000, reward total was -18.000000. running mean: -20.417982\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.413803\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.419665\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.425468\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.431213\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.436901\n",
            "resetting env. episode 1153.000000, reward total was -19.000000. running mean: -20.422532\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.428307\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.434024\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.439683\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.445287\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.450834\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.456325\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.451762\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.457245\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -20.442672\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -20.428245\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.423963\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.429723\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.435426\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.421072\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.426861\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.432592\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -20.428267\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -20.413984\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.419844\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.415646\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.421489\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -20.417274\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.423102\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.428871\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.434582\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.440236\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.435834\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.431475\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.437161\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.442789\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.448361\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.453877\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.459339\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.464745\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.460098\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.465497\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.470842\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.476133\n",
            "resetting env. episode 1192.000000, reward total was -18.000000. running mean: -20.451372\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.456858\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.452290\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.457767\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.453189\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.458657\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.464071\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.469430\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.474736\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.479988\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.475189\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.470437\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.475732\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -20.460975\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.466365\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.471702\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.466985\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.472315\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.477592\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.472816\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.468087\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.463407\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.458773\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -20.454185\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.459643\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -20.445047\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -20.440596\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.446190\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.451728\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.457211\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.462639\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.448012\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.453532\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.458997\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -20.444407\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.449963\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.455463\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.460909\n",
            "resetting env. episode 1230.000000, reward total was -19.000000. running mean: -20.446300\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.451837\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.457318\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.462745\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.468118\n",
            "resetting env. episode 1235.000000, reward total was -18.000000. running mean: -20.443436\n",
            "resetting env. episode 1236.000000, reward total was -18.000000. running mean: -20.419002\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -20.404812\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.410764\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.416656\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.412490\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.408365\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.414281\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.400138\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.406137\n",
            "resetting env. episode 1245.000000, reward total was -17.000000. running mean: -20.372076\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.378355\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.384571\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.380726\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.376918\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.383149\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.389318\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.395424\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.401470\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.397456\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.403481\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.409446\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.415352\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.421198\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.426986\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.432716\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.438389\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -20.424005\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.429765\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.435468\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.431113\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.436802\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.442434\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.438009\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.443629\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.449193\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.454701\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.450154\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.455653\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.461096\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -20.446485\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.442020\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.437600\n",
            "resetting env. episode 1278.000000, reward total was -18.000000. running mean: -20.413224\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.409092\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.405001\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -20.400951\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.406941\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.412872\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.418743\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.414556\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -20.400410\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.406406\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.412342\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.418219\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.424036\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.429796\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.435498\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.441143\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.446732\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.442264\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.447842\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.443363\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.428930\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -20.424640\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.430394\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.436090\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.431729\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.437412\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.433038\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.438707\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.434320\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.439977\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.435577\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.441222\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.446809\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.452341\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.457818\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.453240\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -20.448707\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.454220\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.449678\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.455181\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.460629\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.466023\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -20.451363\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.456849\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.462281\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.467658\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.472981\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.478252\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -20.463469\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -20.458834\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.464246\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.469604\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.464907\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.470258\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.475556\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.480800\n",
            "resetting env. episode 1334.000000, reward total was -19.000000. running mean: -20.465992\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.471332\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.466619\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.461953\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.467333\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.462660\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.468033\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.463353\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.468720\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -20.464032\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.469392\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.474698\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.479951\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.485152\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.490300\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.495397\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.500443\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.505439\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.510384\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.515280\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.510128\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.515026\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.519876\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.524677\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.519431\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.524236\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.528994\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.533704\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.538367\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.532983\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.537653\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.532277\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.526954\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.511685\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.516568\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.521402\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.516188\n",
            "resetting env. episode 1371.000000, reward total was -19.000000. running mean: -20.501026\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.496016\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.491056\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.496145\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.501184\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.496172\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.501210\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.506198\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -20.501136\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.496125\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.481163\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.486352\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.491488\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -20.476573\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.481808\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.486990\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.492120\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.487199\n",
            "resetting env. episode 1389.000000, reward total was -18.000000. running mean: -20.462327\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.447703\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.453226\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.448694\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.434207\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.439865\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.445466\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.451012\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.456502\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.461937\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -20.447317\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.452844\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.448316\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.453832\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.459294\n",
            "resetting env. episode 1404.000000, reward total was -19.000000. running mean: -20.444701\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.450254\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.445752\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.451294\n",
            "resetting env. episode 1408.000000, reward total was -19.000000. running mean: -20.436781\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.442413\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.447989\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.443509\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.449074\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.454583\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.450038\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.455537\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -20.450982\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.456472\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.461907\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.467288\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.472615\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -20.457889\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.463310\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.468677\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -20.463990\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.459351\n",
            "resetting env. episode 1426.000000, reward total was -18.000000. running mean: -20.434757\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.440409\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.446005\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.451545\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -20.437030\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.432660\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.438333\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.443950\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.449510\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.455015\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.450465\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.455960\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.461401\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.456787\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.452219\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.457697\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.453120\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.458588\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.454003\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.459463\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -20.454868\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.460319\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.465716\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.461059\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.466448\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.471784\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.477066\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.472295\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.477572\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.472797\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.478069\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.483288\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -20.468455\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.473771\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.479033\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.484242\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.489400\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.494506\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.489561\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.494665\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.499719\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.504722\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.509674\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.504578\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.509532\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.504437\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.509392\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.514298\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -20.499155\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.504164\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.509122\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.514031\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -20.508891\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.513802\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.508664\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.513577\n",
            "resetting env. episode 1482.000000, reward total was -19.000000. running mean: -20.498441\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.503457\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.508422\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.513338\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -20.508205\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.513123\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.517991\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.512811\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.517683\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.512506\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.517381\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.522208\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -20.516986\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.521816\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.526598\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.531332\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -20.516018\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.520858\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.515649\n",
            "CPU times: user 1h 7min 37s, sys: 30min 51s, total: 1h 38min 28s\n",
            "Wall time: 50min 48s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w2NblmwDsL3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "2a6e9201-16db-4715-da5a-e84f78fa958a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C0854138_LE_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}