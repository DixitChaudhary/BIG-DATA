{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C0854138_LE_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "add0cba5-c145-4f69-ec3f-d3a08c5c0cba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=a3d2fc615cd27707e29d69312b9e4c067fe927b1f39cbba0ec167b5cd241ba23\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "02bc4e52-e4db-47af-ec21-e70f04431d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "5d47c276-4f90-43fa-8089-541263d8aba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "4a268383-aa08-4f8e-a08f-91d6d5a089a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "3c40a2c8-dc62-4ad5-c72b-fe960ef6c730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-6\n",
        "learning_rate = 1e-6\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "e1af7c21-01a6-456d-dcaf-52277cb84ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -19.059105\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.068514\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.087829\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.106951\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.125881\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.144622\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.153176\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.161644\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.180028\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.198227\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -19.196245\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.204283\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.222240\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.230018\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.247717\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.265240\n",
            "resetting env. episode 21.000000, reward total was -18.000000. running mean: -19.252588\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.270062\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.287361\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.294488\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.311543\n",
            "resetting env. episode 26.000000, reward total was -18.000000. running mean: -19.298427\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.315443\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.332289\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.348966\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.365476\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.381821\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.398003\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.414023\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.419883\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.425684\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.441427\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.457013\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.472443\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.477718\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.492941\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.508012\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.522932\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.527702\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.532425\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.547101\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.561630\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.576014\n",
            "resetting env. episode 48.000000, reward total was -18.000000. running mean: -19.560254\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.564651\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -19.569005\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.583315\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.587481\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.601607\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.605591\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.619535\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.633339\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.647006\n",
            "resetting env. episode 58.000000, reward total was -18.000000. running mean: -19.630536\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.634230\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.647888\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.661409\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.674795\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -19.668047\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.671367\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.674653\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -19.677907\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.691127\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.704216\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.707174\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.720102\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.732901\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.725572\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.738317\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.750933\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.763424\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.765790\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.768132\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.780451\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.792646\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.794720\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.796772\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.808805\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.820717\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.832509\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.844184\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.855743\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.867185\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.878513\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.889728\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.900831\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.911823\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.922704\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.933477\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.934143\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.944801\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.955353\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.945800\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.956342\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -19.956778\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.947210\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.947738\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.958261\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.968678\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.968991\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.969302\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.979609\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.989812\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.999914\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.009915\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.009816\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -19.989718\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.999821\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.009822\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.009724\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -19.999627\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.009631\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.019534\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.019339\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.029146\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.028854\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.038566\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.048180\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.057698\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.067121\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.066450\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.075786\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.085028\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.084177\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.083336\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.082502\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.091677\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.070761\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.060053\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.069452\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.068758\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.078070\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.077290\n",
            "resetting env. episode 138.000000, reward total was -17.000000. running mean: -20.046517\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.046051\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.045591\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.055135\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.054584\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.054038\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.043498\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.053063\n",
            "resetting env. episode 146.000000, reward total was -17.000000. running mean: -20.022532\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.032307\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.041984\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.031564\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.041248\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.050836\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.050327\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.059824\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.069226\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.078533\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.087748\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.076871\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.086102\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.085241\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.094388\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.103445\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.112410\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.121286\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.130073\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.138772\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.147385\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.155911\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.164352\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.172708\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.180981\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.189171\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.177280\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.185507\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.193652\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.201715\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.189698\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.187801\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.195923\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.203964\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.211924\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.209805\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.207707\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.205630\n",
            "resetting env. episode 184.000000, reward total was -17.000000. running mean: -20.173574\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.181838\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.180019\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.188219\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.196337\n",
            "resetting env. episode 189.000000, reward total was -18.000000. running mean: -20.174374\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.182630\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.190804\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.188896\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.197007\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.205037\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.212986\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.220856\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.228648\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.226361\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.234098\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.221757\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.219539\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.217344\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.195170\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.203219\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.201186\n",
            "resetting env. episode 206.000000, reward total was -17.000000. running mean: -20.169175\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.167483\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.165808\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.174150\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.172408\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.160684\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.169078\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.177387\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.185613\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.173757\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.182019\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.190199\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.188297\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.196414\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.204450\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.212405\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.210281\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.218179\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.225997\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.233737\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.241399\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.248985\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.256496\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.263931\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.271291\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.278578\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.285793\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.292935\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.300005\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.307005\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.303935\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.300896\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.307887\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.314808\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.321660\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.328443\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.335159\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.331807\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.338489\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.345104\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.341653\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.348237\n",
            "resetting env. episode 248.000000, reward total was -18.000000. running mean: -20.324754\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.331507\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.328192\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.334910\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.341561\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.348145\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.344664\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.351217\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.357705\n",
            "resetting env. episode 257.000000, reward total was -17.000000. running mean: -20.324128\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.330887\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.337578\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.344202\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.350760\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.347252\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.343780\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.340342\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.336939\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.343569\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.350134\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.356632\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.363066\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.359435\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.365841\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.362182\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.368561\n",
            "resetting env. episode 274.000000, reward total was -18.000000. running mean: -20.344875\n",
            "resetting env. episode 275.000000, reward total was -17.000000. running mean: -20.311426\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.298312\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.295329\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.302376\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.309352\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.316258\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.303096\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.310065\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.306964\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.313894\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.320756\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.327548\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.334273\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.320930\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.327720\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.324443\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.321199\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.327987\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.324707\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.321460\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.308245\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.315163\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.322011\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.318791\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.315603\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.322447\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.329223\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.325930\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.332671\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.339344\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.345951\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.352492\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.358967\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.365377\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.371723\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.378006\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.374226\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.370484\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.376779\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.373011\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.369281\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.355588\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.362032\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.348412\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.354928\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.361378\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.367765\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.374087\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.370346\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.366643\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.372976\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.379247\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.375454\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.381700\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.387883\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.394004\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.400064\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.386063\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.372202\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.368480\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.374796\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.381048\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.387237\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.393365\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.399431\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.395437\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.391482\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.397568\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.403592\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.409556\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.415460\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.421306\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.417093\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.412922\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -20.388793\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.374905\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.381156\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.387344\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.373471\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.369736\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.376039\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.372278\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.378555\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.384770\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.380922\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.387113\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.383242\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.379409\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.375615\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.381859\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.388041\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.374160\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.380419\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.376614\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.382848\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.379020\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.375230\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.371477\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.377762\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.373985\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.380245\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.376443\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.382678\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.388851\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.394963\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.401013\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.397003\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.403033\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.409003\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.414913\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.420764\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.426556\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.432290\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.427967\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.433688\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.429351\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.415057\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.420907\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.426698\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.432431\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.428106\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.433825\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.439487\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.435092\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.440741\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.446334\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.441871\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.427452\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.423177\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.418946\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.424756\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.430509\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.406204\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.412141\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.418020\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.423840\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.419601\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.425405\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.411151\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.417040\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.412869\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.418741\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.424553\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.430308\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.436005\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.441645\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.437228\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.442856\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.448427\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.453943\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.459404\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.464810\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.460162\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.455560\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.451004\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.446494\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.432029\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.437709\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.423332\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.429099\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.404808\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.410760\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.416652\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.422486\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.418261\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.424078\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.429837\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.435539\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.441183\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.446772\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.452304\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.457781\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.463203\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.468571\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.473885\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.469147\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.464455\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.469810\n",
            "resetting env. episode 453.000000, reward total was -18.000000. running mean: -20.445112\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.420661\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.426455\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.422190\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.427968\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.423689\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.419452\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.425257\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.431005\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.426694\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.422428\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.428203\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.413921\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.419782\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.425584\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.431328\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.417015\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.412845\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.418716\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.424529\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.430284\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.415981\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.411821\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.417703\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.413526\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.419391\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.415197\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.421045\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.426835\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.432566\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.418241\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.424058\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.409818\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.415719\n",
            "resetting env. episode 487.000000, reward total was -18.000000. running mean: -20.391562\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.397647\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.403670\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.409633\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.415537\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.421382\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.417168\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.422996\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.418766\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.424579\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.430333\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.436029\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.441669\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.417252\n",
            "CPU times: user 29min 34s, sys: 11min 58s, total: 41min 33s\n",
            "Wall time: 21min 30s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "3967431a-426d-465b-dc05-61739bfa4207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.970398\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.960694\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.961087\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.941476\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.932061\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.912741\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.903613\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.904577\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.895532\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.886576\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.877710\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.878933\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.880144\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.881343\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.882529\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.883704\n",
            "resetting env. episode 26.000000, reward total was -17.000000. running mean: -20.844867\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.846418\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.837954\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.839574\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.831179\n",
            "resetting env. episode 31.000000, reward total was -18.000000. running mean: -20.802867\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.794838\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.796890\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.798921\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.800932\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.802922\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.804893\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.796844\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.798876\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.790887\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.782978\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.775148\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.777397\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.779623\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.781827\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.784008\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.786168\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.778307\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.780524\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.782718\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.774891\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.767142\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.769471\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.751776\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.754258\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.746716\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.739249\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.741856\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.734438\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.737093\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.719722\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.722525\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.725300\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.728047\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.730766\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.723459\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.726224\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.708962\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.701872\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.694854\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.677905\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.671126\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.674415\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.677671\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.680894\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.684085\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.687244\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.690372\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.683468\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.686633\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.689767\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.692869\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.695941\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.698981\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.681991\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.685171\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.688320\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.671436\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.664722\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.668075\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.651394\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.654880\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.658331\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.661748\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.665131\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.668479\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.661794\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.665177\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.668525\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.661840\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.655221\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.658669\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.642082\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.645661\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.639205\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.632813\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.626485\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.620220\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.624018\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.607777\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.611700\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.615583\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.619427\n",
            "resetting env. episode 114.000000, reward total was -18.000000. running mean: -20.593233\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.577300\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.581527\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.585712\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.589855\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.593956\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.588017\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.592137\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.596215\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.600253\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.604251\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.608208\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.612126\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.606005\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.599945\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.593945\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.598006\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.592026\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.596105\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.590144\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.594243\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.588300\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.582417\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.576593\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.580827\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.585019\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.589169\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.593277\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.597344\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.601371\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.595357\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.599404\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.603410\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.607376\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.611302\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.615189\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.619037\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.622847\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.616618\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.620452\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.624247\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.628005\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.611725\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.605608\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.609552\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.613456\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.607321\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.611248\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.605136\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.599084\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.593094\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.597163\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.601191\n",
            "resetting env. episode 167.000000, reward total was -17.000000. running mean: -20.565179\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.569527\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.573832\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.568094\n",
            "resetting env. episode 171.000000, reward total was -17.000000. running mean: -20.532413\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.537089\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.541718\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.546301\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.550838\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.555329\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.559776\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.564178\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.558536\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.552951\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.557421\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.541847\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.546429\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.540965\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.535555\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.520199\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.514997\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.509847\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.514749\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.509601\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -20.484505\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.469660\n",
            "resetting env. episode 193.000000, reward total was -18.000000. running mean: -20.444964\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.450514\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.446009\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.451549\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.457033\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.462463\n",
            "resetting env. episode 199.000000, reward total was -18.000000. running mean: -20.437838\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.443460\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.449025\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.454535\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.449990\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.445490\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.451035\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.456525\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.461959\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.457340\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.462766\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.468139\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.463457\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.468823\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.474135\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.479393\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.484599\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.489753\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.494856\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.499907\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.494908\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.489959\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.485059\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.490209\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.495307\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.500354\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.495350\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.500397\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.505393\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.510339\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.515235\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.520083\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.514882\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.519733\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.514536\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.519391\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.514197\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.499055\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.504064\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.509024\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.513933\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.508794\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.513706\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.518569\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.523383\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.518150\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.512968\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.517838\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.512660\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.507533\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.512458\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.517333\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.512160\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.517039\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.521868\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.526649\n",
            "resetting env. episode 255.000000, reward total was -17.000000. running mean: -20.491383\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.496469\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.501504\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.506489\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.511424\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.516310\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.521147\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.505936\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.510876\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.515768\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.520610\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.525404\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.530150\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.534848\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.529500\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.524205\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.528963\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.533673\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.508336\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.513253\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.508120\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.493039\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.498109\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.503128\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.508097\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.513016\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.517885\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.512707\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.517579\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.502404\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.507380\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.502306\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.507283\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.512210\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.517088\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.521917\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.526698\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.531431\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.536117\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.520755\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.525548\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.530292\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.534989\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.539640\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.544243\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.548801\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.553313\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.557780\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.562202\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.556580\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.561014\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.565404\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.559750\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.564152\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.568511\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.562826\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.557197\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.561625\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.566009\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.570349\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.574646\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.578899\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.583110\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.587279\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.591406\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.565492\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.569837\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.574139\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.568397\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.562714\n",
            "resetting env. episode 325.000000, reward total was -18.000000. running mean: -20.537086\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.541716\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.536298\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.540935\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.545526\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.540071\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.544670\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.529223\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.533931\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.538592\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.533206\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.527874\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.532595\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.537269\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.521896\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.526677\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.531411\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.526097\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.530836\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.535527\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.530172\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.514870\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.509722\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.504624\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.509578\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.504482\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.499438\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.504443\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.499399\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.504405\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.489361\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.474467\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.479722\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.474925\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.480176\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.475374\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.470620\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.475914\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.481155\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.476344\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.461580\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.466964\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.472295\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.477572\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.482796\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.487968\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.493088\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.498157\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.503176\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.508144\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.493063\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.488132\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.493251\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.498318\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.483335\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.488502\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.493617\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.498681\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.493694\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.498757\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.503769\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.508732\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.503644\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.508608\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.513522\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.518386\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.503203\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.508171\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.513089\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.517958\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.522778\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.527551\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.512275\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.517152\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.511981\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.506861\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.511792\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.506674\n",
            "resetting env. episode 403.000000, reward total was -17.000000. running mean: -20.471608\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.476892\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.472123\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.477402\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.482628\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.477801\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.483023\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.488193\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.473311\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.468578\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.463892\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.469253\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.454561\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.450015\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.455515\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.460960\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.446350\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.451887\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.457368\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.462794\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.448166\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.453685\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.459148\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.464556\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.469911\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.475212\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.470459\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.475755\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.480997\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.466187\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.471525\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.476810\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.482042\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.487222\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.492349\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.487426\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.492552\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.497626\n",
            "resetting env. episode 441.000000, reward total was -18.000000. running mean: -20.472650\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.477923\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.483144\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.488313\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.493430\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.498495\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.493510\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.488575\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.493690\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.498753\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.503765\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.498727\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.503740\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.508703\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.513616\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.508480\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.503395\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.508361\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.513277\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.518144\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.522963\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.527733\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.532456\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.537131\n",
            "resetting env. episode 465.000000, reward total was -17.000000. running mean: -20.501760\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.506743\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.491675\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.496758\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.501791\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.496773\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.471805\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.477087\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.482316\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.487493\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.492618\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.497692\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.492715\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.497788\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.502810\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.507782\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.502704\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.487677\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.492800\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.497872\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.482894\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.478065\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.473284\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.468551\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.473866\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.469127\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.474436\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.459691\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.465094\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.460443\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.455839\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.461281\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.456668\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.462101\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.467480\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.472805\n",
            "CPU times: user 30min 55s, sys: 12min 1s, total: 42min 56s\n",
            "Wall time: 22min 7s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "0f228f4e-66be-4a37-d5e3-ca45fab77e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "9fb75293-6068-4a4d-9dbc-e9428799a58e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -20.950398\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.950894\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.951385\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951871\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.942353\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.932929\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.923600\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.924364\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.925120\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.905869\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.906810\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.907742\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.888665\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.889778\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.890880\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.881971\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.873152\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.864420\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.865776\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.867118\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.868447\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.869763\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.861065\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.862454\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.863830\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.865191\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.866540\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.867874\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.859195\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.850603\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.842097\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.843676\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.845240\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.836787\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.828419\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.830135\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.831834\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.833516\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.825180\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.826929\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.808659\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.790573\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.792667\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.784740\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.786893\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.789024\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.791134\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.783222\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.785390\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.777536\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.779761\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.771963\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.774244\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.776501\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.758736\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.751149\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.753637\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.756101\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.748540\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.751055\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.753544\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.756009\n",
            "resetting env. episode 67.000000, reward total was -18.000000. running mean: -20.728449\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.731164\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.733852\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.726514\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.719249\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.722056\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.724836\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.717587\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.720411\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.703207\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.696175\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.699213\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.702221\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.695199\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.698247\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.701265\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.704252\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.707210\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.700137\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.703136\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.686105\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.689244\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.692351\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.695428\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.698473\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.691489\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.694574\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.697628\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.700652\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.693645\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.696709\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.699742\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.682744\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.685917\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.689058\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.692167\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.695245\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.698293\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.691310\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.694397\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.687453\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.690578\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.693673\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.696736\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.699769\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.682771\n",
            "resetting env. episode 113.000000, reward total was -18.000000. running mean: -20.655943\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.639384\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.632990\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.636660\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.640293\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.613890\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.617752\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.611574\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.605458\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.599404\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.593410\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.597476\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.601501\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.605486\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.599431\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.603437\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.597402\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.601428\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.605414\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.609360\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.613266\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.607134\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.611062\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.614952\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -20.588802\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.582914\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.587085\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.581214\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.585402\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.569548\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.563852\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.568214\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.552532\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.547006\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.541536\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.546121\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.550660\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.535153\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.539802\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.544404\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.548960\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.553470\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.557935\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.562356\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.566732\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.561065\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.565454\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.569800\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.574102\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.578361\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.572577\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.576852\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.561083\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.565472\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.569817\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.574119\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.578378\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.572594\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.576868\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.581100\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.585289\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.579436\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.583641\n",
            "resetting env. episode 176.000000, reward total was -18.000000. running mean: -20.557805\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.562227\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.566605\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.560939\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.555329\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.559776\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.534178\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.538836\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.543448\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.548014\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.552533\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.557008\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.561438\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.565824\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.570165\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.564464\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.568819\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.553131\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.557600\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.562024\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.566403\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.560739\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.565132\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.559481\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.563886\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.538247\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.532865\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.537536\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.542161\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.536739\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.531372\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.536058\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.530697\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.535390\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.540036\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.534636\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.539290\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.523897\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.528658\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.533371\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.528037\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.532757\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.537430\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.542055\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.546635\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.541168\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.535757\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.540399\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.544995\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.519545\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.514350\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.519206\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.494014\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.499074\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.504083\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.509042\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.513952\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.498812\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.503824\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.498786\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.493798\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.478860\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.484072\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.479231\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.484439\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.469594\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.474898\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.480149\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.485348\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.490494\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.495589\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.480634\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.485827\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.490969\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.496059\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.501099\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.496088\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.491127\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.496215\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.501253\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.506241\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.511178\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.516067\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.500906\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.495897\n",
            "resetting env. episode 261.000000, reward total was -17.000000. running mean: -20.460938\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.466329\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.471665\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.476949\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.462179\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.447557\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.453082\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.458551\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.453965\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.449426\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.454932\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.460382\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.465778\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.471121\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.466409\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.461745\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.467128\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.472457\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.477732\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.472955\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.468225\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.453543\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.449007\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.444517\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.430072\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.435771\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.431414\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.427100\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.432829\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.438500\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.424115\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.429874\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.435575\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.421220\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.427007\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.422737\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.428510\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.434225\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.429883\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.435584\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.441228\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.436816\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.442448\n",
            "resetting env. episode 304.000000, reward total was -17.000000. running mean: -20.408023\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.413943\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.419803\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.415605\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.401449\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.397435\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.393461\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.399526\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.405531\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.391475\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.397561\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.403585\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.399549\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.405554\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.411498\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.407383\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.413309\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.419176\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.424984\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.430735\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.416427\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.422263\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.428040\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.433760\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.429422\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.435128\n",
            "resetting env. episode 330.000000, reward total was -18.000000. running mean: -20.410777\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.416669\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.422502\n",
            "resetting env. episode 333.000000, reward total was -18.000000. running mean: -20.398277\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.404295\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.410252\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.416149\n",
            "resetting env. episode 337.000000, reward total was -17.000000. running mean: -20.381988\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.378168\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.384386\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.390542\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.386637\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.392770\n",
            "resetting env. episode 343.000000, reward total was -18.000000. running mean: -20.368843\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.355154\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.361603\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.367987\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.374307\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.360564\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.356958\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.363389\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.369755\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.376057\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.372297\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.378574\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.364788\n",
            "resetting env. episode 356.000000, reward total was -16.000000. running mean: -20.321140\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.317929\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.314749\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.321602\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.318386\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.325202\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.331950\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.338630\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.325244\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.321992\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.328772\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.335484\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.342129\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.338708\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.325321\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.332068\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.318747\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.305559\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.312504\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.319379\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.326185\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.332923\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.329594\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.336298\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.342935\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.349506\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.346011\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.342551\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.339125\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.345734\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.352276\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.348754\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.355266\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.361713\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.348096\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.344615\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.351169\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.357658\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.354081\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.360540\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.366935\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.373265\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.369533\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.375837\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.382079\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.388258\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.394376\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.390432\n",
            "resetting env. episode 404.000000, reward total was -18.000000. running mean: -20.366528\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.372862\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.379134\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.385342\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.391489\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.397574\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.403598\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.409562\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.405467\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.391412\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.397498\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.393523\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.399588\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.405592\n",
            "resetting env. episode 418.000000, reward total was -18.000000. running mean: -20.381536\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.377721\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.383943\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.380104\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.386303\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.392440\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.388515\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.394630\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.400684\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.406677\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.412610\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.418484\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.424299\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.430056\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.435756\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.441398\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.426984\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.432714\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.438387\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.424003\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.419763\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.425566\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.421310\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.427097\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.432826\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.428498\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.414213\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.420071\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.425870\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.421611\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.417395\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.423221\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.418989\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.414799\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.420651\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.416445\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.412280\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.398157\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.404176\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.410134\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.416033\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.411872\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.417754\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.403576\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.409540\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -20.385445\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.371591\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.377875\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.374096\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.380355\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.386551\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -20.362686\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.349059\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.345568\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.332113\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.338792\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.345404\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.351950\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.348430\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.354946\n",
            "resetting env. episode 478.000000, reward total was -17.000000. running mean: -20.321396\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.308182\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.295101\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.302150\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.309128\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.296037\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.303076\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.300046\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.307045\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.313975\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.310835\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.307727\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.314649\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.311503\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.318388\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.325204\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.321952\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.308732\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.305645\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.312589\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.319463\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.326268\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.333005\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.339675\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.336279\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.342916\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.349487\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.355992\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.362432\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.358808\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.365220\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.361567\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.367952\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.374272\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.380529\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.386724\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -20.372857\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.369128\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.375437\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.371683\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.377966\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.384186\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.390344\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.396441\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.402476\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.388452\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.384567\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.390722\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.386814\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -20.372946\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.379217\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.375425\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.381670\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.387854\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.393975\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.400035\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.406035\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.411975\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.407855\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -20.393776\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.399839\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.395840\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.391882\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.397963\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.393983\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.390043\n",
            "resetting env. episode 544.000000, reward total was -16.000000. running mean: -20.346143\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.332682\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.329355\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.336061\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.342701\n",
            "resetting env. episode 549.000000, reward total was -19.000000. running mean: -20.329274\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.335981\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.342621\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.349195\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.355703\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.362146\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.358524\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.364939\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.371290\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.377577\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.383801\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -20.369963\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -20.356263\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.352701\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.359174\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.365582\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.371926\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.378207\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.384425\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -20.370581\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.366875\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -20.363206\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.359574\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.355978\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.362419\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.368794\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.375106\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.381355\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.387542\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.393666\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.389730\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.385832\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.371974\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.368254\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.374572\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.380826\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.387018\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.383148\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.379316\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.385523\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.391668\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.397751\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.403774\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.409736\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.415639\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.421482\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.427267\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.422995\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.408765\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -20.394677\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.390730\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.396823\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.402855\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.408826\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.414738\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.420591\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.416385\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.412221\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.408099\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.404018\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.389977\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.396078\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.402117\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.408096\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.404015\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.399975\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.395975\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.402015\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.407995\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.413915\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.409776\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.415678\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.411521\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.417406\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.423232\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.429000\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.434710\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.430363\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.436059\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.441698\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.447281\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.452809\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.448281\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.453798\n",
            "resetting env. episode 633.000000, reward total was -19.000000. running mean: -20.439260\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.444867\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.450418\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.455914\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.461355\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.466742\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.472074\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.477353\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.472580\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.477854\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.483076\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.478245\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.483462\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -20.468628\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.473941\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.479202\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.464410\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.469766\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.475068\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.480318\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.485514\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.490659\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.495753\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.500795\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.495787\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.500829\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.505821\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.510763\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.515655\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.500499\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.505494\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.500439\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.505434\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.510380\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.495276\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.490323\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.495420\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.500466\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.505461\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.510407\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.515303\n",
            "resetting env. episode 674.000000, reward total was -19.000000. running mean: -20.500150\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.495148\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.500197\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.495195\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.500243\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.505240\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.500188\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.505186\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.510134\n",
            "resetting env. episode 683.000000, reward total was -17.000000. running mean: -20.475033\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.470282\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.475580\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.480824\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.486016\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.491155\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.496244\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.501281\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.486269\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.481406\n",
            "resetting env. episode 693.000000, reward total was -16.000000. running mean: -20.436592\n",
            "resetting env. episode 694.000000, reward total was -19.000000. running mean: -20.422226\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.428004\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.433724\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.439386\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.434993\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.440643\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.436236\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.441874\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.437455\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.433081\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.428750\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -20.414462\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.420318\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.426114\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.421853\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.427635\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.423358\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.419125\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.414934\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.410784\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.406676\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.402610\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.408584\n",
            "resetting env. episode 717.000000, reward total was -19.000000. running mean: -20.394498\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.400553\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.396547\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.392582\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.388656\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.394769\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -20.390822\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.386913\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.383044\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.379214\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.375422\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.381668\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.387851\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.393972\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.380033\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.386232\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.392370\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.388446\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.394562\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.390616\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.396710\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.402743\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.408716\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.414628\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.420482\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -20.406277\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.402214\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.408192\n",
            "resetting env. episode 745.000000, reward total was -18.000000. running mean: -20.384110\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.380269\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.386467\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.392602\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.398676\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.394689\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.400742\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.396735\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.402768\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.408740\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.404652\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.410606\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.416500\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.422335\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.428112\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.423830\n",
            "resetting env. episode 761.000000, reward total was -18.000000. running mean: -20.399592\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.405596\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.401540\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.407525\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -20.393450\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.379515\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.385720\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.391863\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.397944\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.403965\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.399925\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.405926\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.411866\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.417748\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.423570\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.429335\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.435041\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -20.420691\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.416484\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.422319\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.428096\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.433815\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.439477\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -20.425082\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.430831\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.426523\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.432258\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.437935\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.443556\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.449120\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.454629\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.460083\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.465482\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.470827\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.476119\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.481358\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.486544\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.491679\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.496762\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.501794\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.506776\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -20.491708\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.496791\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.491823\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.486905\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.492036\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -20.477116\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -20.462345\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.467721\n",
            "resetting env. episode 810.000000, reward total was -18.000000. running mean: -20.443044\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.448614\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.444127\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.439686\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.445289\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.450836\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.456328\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.461765\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.467147\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.472476\n",
            "resetting env. episode 820.000000, reward total was -18.000000. running mean: -20.447751\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -20.443273\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.448841\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.454352\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.459809\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.455211\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.450659\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.446152\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.441690\n",
            "resetting env. episode 829.000000, reward total was -19.000000. running mean: -20.427274\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.433001\n",
            "resetting env. episode 831.000000, reward total was -18.000000. running mean: -20.408671\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.414584\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.420438\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.416234\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.422072\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.427851\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.423572\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.429337\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.425043\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.420793\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.426585\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -20.422319\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.408096\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.414015\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.409875\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.415776\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.421618\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.427402\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.433128\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.438797\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.434409\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.440065\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.445664\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.441207\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.446795\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.452327\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.447804\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.443326\n",
            "resetting env. episode 859.000000, reward total was -19.000000. running mean: -20.428893\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.434604\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.430258\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.425955\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -20.401696\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.397679\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.393702\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.399765\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.405767\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.411710\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.417592\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.423417\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.429182\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.434891\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -20.420542\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -20.406336\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.412273\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.418150\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.423969\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.419729\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.425532\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.431276\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.436964\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.442594\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.438168\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.433786\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.429448\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.435154\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.440802\n",
            "resetting env. episode 888.000000, reward total was -17.000000. running mean: -20.406394\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.412330\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.418207\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.424025\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.419785\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.425587\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.431331\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.427018\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.432748\n",
            "resetting env. episode 897.000000, reward total was -20.000000. running mean: -20.428420\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.434136\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.439795\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.445397\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.450943\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.446433\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -20.431969\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.427649\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.433373\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.439039\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.424649\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.430402\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.426098\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.421837\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -20.407619\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.413543\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.419407\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.425213\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.430961\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.436651\n",
            "resetting env. episode 917.000000, reward total was -18.000000. running mean: -20.412285\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.408162\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.414080\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.419940\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -20.405740\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.411683\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.407566\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.413490\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.419355\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.425162\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.430910\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.426601\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.432335\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.438012\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.443632\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.449195\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.434703\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.420356\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.416153\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.411991\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.417871\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.413693\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.409556\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.415460\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.421306\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.427092\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.412822\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.408693\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.414606\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.410460\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.406356\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.412292\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.418169\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.423988\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.429748\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.425450\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.421196\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.426984\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.422714\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.428487\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.434202\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.429860\n",
            "resetting env. episode 959.000000, reward total was -19.000000. running mean: -20.415561\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.421406\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.417192\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.423020\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.428790\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.424502\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.430257\n",
            "resetting env. episode 966.000000, reward total was -17.000000. running mean: -20.395954\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.391994\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.398075\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.404094\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.400053\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.406052\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -20.391992\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.398072\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.404091\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.410050\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.405950\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.411890\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.417771\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.423594\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.429358\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.435064\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.440713\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.446306\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.451843\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.457325\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.462752\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.468124\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.473443\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.468708\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.464021\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.459381\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.464787\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.470139\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.475438\n",
            "resetting env. episode 995.000000, reward total was -18.000000. running mean: -20.450684\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.456177\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.461615\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.466999\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.472329\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.467606\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.472930\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.468200\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.463518\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.468883\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.474194\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.479452\n",
            "resetting env. episode 1007.000000, reward total was -16.000000. running mean: -20.434658\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.430311\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.436008\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.441648\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.437232\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.442859\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.448431\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.443946\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -20.429507\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.435212\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.430860\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.426551\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -20.412286\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.418163\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.423981\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.429741\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.425444\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.431189\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.436878\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.442509\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.438084\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.443703\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.449266\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.454773\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.460225\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.465623\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.470967\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.466257\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.471595\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.476879\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.482110\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.477289\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.482516\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.487691\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.492814\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.497886\n",
            "resetting env. episode 1043.000000, reward total was -18.000000. running mean: -20.472907\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.468178\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.463496\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -20.448861\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.454372\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.459829\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.465230\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.460578\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.455972\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.461413\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -20.446799\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.452331\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.457807\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.463229\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.468597\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -20.453911\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.459372\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.464778\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.460130\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.465529\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.460874\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.446265\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -20.441802\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.437384\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.433010\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.428680\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.434394\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.430050\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.435749\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.441392\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.446978\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.452508\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.447983\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.453503\n",
            "resetting env. episode 1077.000000, reward total was -20.000000. running mean: -20.448968\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.444478\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.450034\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.455533\n",
            "resetting env. episode 1081.000000, reward total was -18.000000. running mean: -20.430978\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.426668\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.432401\n",
            "resetting env. episode 1084.000000, reward total was -18.000000. running mean: -20.408077\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.413997\n",
            "resetting env. episode 1086.000000, reward total was -18.000000. running mean: -20.389857\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.395958\n",
            "resetting env. episode 1088.000000, reward total was -18.000000. running mean: -20.371999\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.368279\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.374596\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.380850\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.387041\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.383171\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.369339\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.375646\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -20.361889\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.358270\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.354688\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.361141\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.357529\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.353954\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -20.330415\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.337110\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.333739\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.340402\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.336998\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.333628\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.340292\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -20.326889\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.323620\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.330384\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.337080\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.343709\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.350272\n",
            "resetting env. episode 1115.000000, reward total was -19.000000. running mean: -20.336769\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.343402\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.349968\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.346468\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.353003\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.359473\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.355878\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.362320\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.368696\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.375009\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.381259\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.387447\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.393572\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.399637\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.405640\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.411584\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.417468\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.423293\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.429060\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.424770\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.420522\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.426317\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.432054\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.427733\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.433456\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.429121\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.434830\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.440482\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.446077\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.451616\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.457100\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.462529\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.467904\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.473225\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.468492\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.463807\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.469169\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.474478\n",
            "resetting env. episode 1153.000000, reward total was -18.000000. running mean: -20.449733\n",
            "resetting env. episode 1154.000000, reward total was -19.000000. running mean: -20.435236\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.440883\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.446474\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.452010\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -20.447490\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.453015\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.458485\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.443900\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.449461\n",
            "resetting env. episode 1163.000000, reward total was -18.000000. running mean: -20.424966\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.430716\n",
            "resetting env. episode 1165.000000, reward total was -18.000000. running mean: -20.406409\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.412345\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.418222\n",
            "resetting env. episode 1168.000000, reward total was -17.000000. running mean: -20.384039\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.380199\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.386397\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.382533\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.388708\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.384821\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.390973\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.397063\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.403092\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.389061\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.395171\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.401219\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.407207\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.403135\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.409103\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.415012\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -20.410862\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.406754\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.412686\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.418559\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.424374\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.430130\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.435829\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.441470\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.437056\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.442685\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.448258\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.453776\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.449238\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.444745\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.440298\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.445895\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.451436\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.436922\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.442552\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.448127\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.453646\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -20.439109\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.444718\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.450271\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.455768\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.451211\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.456698\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.462131\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -20.447510\n",
            "resetting env. episode 1213.000000, reward total was -18.000000. running mean: -20.423035\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.428805\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.434517\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -20.440171\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.435770\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.441412\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.446998\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.442528\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.438103\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.443722\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.449284\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.444792\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.440344\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.445940\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.451481\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.456966\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.462396\n",
            "resetting env. episode 1230.000000, reward total was -19.000000. running mean: -20.447772\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.443295\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.438862\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.444473\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.440028\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -20.425628\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.431372\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -20.417058\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.422888\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.428659\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.424372\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.430128\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.435827\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.441469\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.447054\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.452584\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -20.438058\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.433677\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.439340\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.434947\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.430598\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.426292\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.422029\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.417808\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.423630\n",
            "resetting env. episode 1255.000000, reward total was -18.000000. running mean: -20.399394\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.405400\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.401346\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -20.387333\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.393459\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.389525\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.395629\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.401673\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.407656\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -20.393580\n",
            "resetting env. episode 1265.000000, reward total was -18.000000. running mean: -20.369644\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -20.365948\n",
            "resetting env. episode 1267.000000, reward total was -18.000000. running mean: -20.342288\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.338865\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.345477\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.352022\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.348502\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.345017\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.351566\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -20.338051\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.334670\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.331324\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.338010\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.334630\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.341284\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.337871\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.344492\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.351047\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.357537\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.363962\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.370322\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.376619\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.362853\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.369224\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.375532\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.371776\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.378059\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.384278\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.390435\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.386531\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.392666\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.398739\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.384752\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.390904\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -20.376995\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -20.363225\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.369593\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.365897\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.372238\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.378516\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.384730\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.380883\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -20.367074\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.353404\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.359869\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.366271\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.372608\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.378882\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.385093\n",
            "resetting env. episode 1314.000000, reward total was -19.000000. running mean: -20.371242\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.367530\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.373855\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.380116\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.376315\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.382552\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.388726\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.384839\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.390991\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.397081\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.403110\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.399079\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.405088\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.411037\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.416927\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.422757\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.428530\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -20.414245\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -20.400102\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.406101\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.412040\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.417920\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -20.403740\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.399703\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -20.395706\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.391749\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.397831\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.403853\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -20.389815\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.395916\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.391957\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.398038\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.394057\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.400117\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.406116\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.412054\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.417934\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.423755\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.419517\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.425322\n",
            "resetting env. episode 1354.000000, reward total was -19.000000. running mean: -20.411069\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.406958\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.412888\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.418760\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.424572\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.430326\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.436023\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -20.421663\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.427446\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.423172\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.418940\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.424750\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.430503\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.436198\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.431836\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.437518\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.443142\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.448711\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.444224\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.439782\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.445384\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.450930\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.446421\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.451957\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.457437\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.462863\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.468234\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.453552\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -20.449016\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.444526\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.450081\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.445580\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.451124\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -20.436613\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.442247\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.437824\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.433446\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.439112\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.434720\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.430373\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.426069\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.421809\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.427591\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.423315\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.419082\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.424891\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.430642\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.436335\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.441972\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.447552\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.443077\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.448646\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.444160\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.449718\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.455221\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.450669\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -20.446162\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.441700\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.447283\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.452811\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.458282\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.463700\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.469063\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.474372\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -20.469628\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.464932\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.460283\n",
            "resetting env. episode 1421.000000, reward total was -18.000000. running mean: -20.435680\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.441323\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.436910\n",
            "resetting env. episode 1424.000000, reward total was -18.000000. running mean: -20.412541\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.408415\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.414331\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.420188\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -20.415986\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.421826\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.417608\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.423432\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.429197\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.424905\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.430656\n",
            "resetting env. episode 1435.000000, reward total was -15.000000. running mean: -20.376350\n",
            "resetting env. episode 1436.000000, reward total was -17.000000. running mean: -20.342586\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.349160\n",
            "resetting env. episode 1438.000000, reward total was -19.000000. running mean: -20.335669\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -20.322312\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.329089\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.335798\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.342440\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -20.339016\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.335626\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.342269\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -20.338847\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.345458\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.342004\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.348584\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.355098\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -20.351547\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.358031\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.364451\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.370806\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.377098\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.383327\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.389494\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.395599\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.391643\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.387727\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.393850\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.399911\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.405912\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.411853\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.407734\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.403657\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -20.389620\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.395724\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.381767\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.387949\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.384070\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -20.370229\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.366527\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.372862\n",
            "resetting env. episode 1475.000000, reward total was -18.000000. running mean: -20.349133\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.345642\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -20.332185\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.338863\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.345475\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.342020\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.338600\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.345214\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.351762\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.358244\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -20.354662\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -20.351115\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.357604\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.364028\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.370387\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.366684\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.373017\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.379287\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.385494\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.391639\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.397722\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.393745\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.399808\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.405810\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.411752\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -20.397634\n",
            "CPU times: user 1h 30min 47s, sys: 36min 13s, total: 2h 7min 1s\n",
            "Wall time: 1h 5min 54s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "6db02bdb-3f4e-4e0e-9afc-c8656f724c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}